{"1": {"level": 1, "name": "Introduction", "child_edge": [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46], "parent_edge": -1, "size": 990}, "2": {"level": 1, "name": "Related Work", "child_edge": [3, 4, 5], "parent_edge": -1, "size": 1020}, "3": {"level": 2, "name": "Recognition of Social Contexts", "parent_edge": 2, "child_edge": [47, 48], "size": 250}, "4": {"level": 2, "name": "Technique with First-Person View Video", "parent_edge": 2, "child_edge": [49, 50], "size": 246}, "5": {"level": 2, "name": "Utilization of Captured Experiential Data", "parent_edge": 2, "child_edge": [51, 52, 53, 54, 55, 56, 57], "size": 524}, "6": {"level": 1, "name": "Social Activity Measurement", "child_edge": [7, 8], "parent_edge": -1, "size": 900}, "7": {"level": 2, "name": "Requirements", "parent_edge": 6, "child_edge": [58], "size": 113}, "8": {"level": 2, "name": "Implementation", "parent_edge": 6, "child_edge": [59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92], "size": 787}, "9": {"level": 1, "name": "Subjective Evaluation Experiment", "child_edge": [10, 11, 12, 13], "parent_edge": -1, "size": 2181}, "10": {"level": 2, "name": "Data Collection", "parent_edge": 9, "child_edge": [93, 94, 95, 96, 97], "size": 169}, "11": {"level": 2, "name": "Experiment Participant", "parent_edge": 9, "child_edge": [98, 99, 100, 101, 102], "size": 117}, "12": {"level": 2, "name": "Experimental Procedure", "parent_edge": 9, "child_edge": [103, 104], "size": 200}, "13": {"level": 2, "name": "Results of the Experiment", "parent_edge": 9, "child_edge": [105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153], "size": 1695}, "14": {"level": 1, "name": "Discussion", "child_edge": [15, 16, 17, 18], "parent_edge": -1, "size": 635}, "15": {"level": 2, "name": "Active Face-to-Face Engagement", "parent_edge": 14, "child_edge": [154], "size": 128}, "16": {"level": 2, "name": "Consistency of Evaluation by Evaluator", "parent_edge": 14, "child_edge": [155, 156, 157, 158], "size": 88}, "17": {"level": 2, "name": "Consistency of Evaluation by Type of Scene", "parent_edge": 14, "child_edge": [159], "size": 133}, "18": {"level": 2, "name": "Field of View of First-Person View Camera", "parent_edge": 14, "child_edge": [160, 161, 162], "size": 286}, "19": {"level": 1, "name": "Conclusion", "child_edge": [163, 164, 165, 166, 167], "parent_edge": -1, "size": 281}, "20": {"level": 1, "name": "Acknowledgments", "child_edge": [168], "parent_edge": -1, "size": 28}, "21": {"level": 1, "name": "References", "parent_edge": -1, "child_edge": [169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193], "size": 1236}, "22": {"level": 3, "pid": 0, "parent_edge": 0, "size": 12}, "23": {"level": 3, "pid": 1, "parent_edge": 0, "size": 11}, "24": {"level": 3, "pid": 2, "parent_edge": 0, "size": 11}, "25": {"level": 3, "pid": 3, "parent_edge": 0, "size": 14}, "26": {"level": 3, "pid": 4, "parent_edge": 0, "size": 303}, "27": {"level": 3, "pid": 5, "parent_edge": 0, "size": 142}, "28": {"level": 3, "pid": 6, "parent_edge": 0, "size": 66}, "29": {"level": 3, "pid": 7, "parent_edge": 0, "size": 25}, "30": {"level": 3, "pid": 8, "parent_edge": 0, "size": 19}, "31": {"level": 3, "pid": 9, "parent_edge": 0, "size": 61}, "32": {"level": 3, "pid": 10, "parent_edge": 1, "size": 65}, "33": {"level": 3, "pid": 11, "parent_edge": 1, "size": 92}, "34": {"level": 3, "pid": 12, "parent_edge": 1, "size": 11}, "35": {"level": 3, "pid": 13, "parent_edge": 1, "size": 5}, "36": {"level": 3, "pid": 14, "parent_edge": 1, "size": 15}, "37": {"level": 3, "pid": 15, "parent_edge": 1, "size": 179}, "38": {"level": 3, "pid": 16, "parent_edge": 1, "size": 163}, "39": {"level": 3, "pid": 17, "parent_edge": 1, "size": 155}, "40": {"level": 3, "pid": 18, "parent_edge": 1, "size": 22}, "41": {"level": 3, "pid": 19, "parent_edge": 1, "size": 24}, "42": {"level": 3, "pid": 20, "parent_edge": 1, "size": 62}, "43": {"level": 3, "pid": 21, "parent_edge": 1, "size": 75}, "44": {"level": 3, "pid": 22, "parent_edge": 1, "size": 55}, "45": {"level": 3, "pid": 23, "parent_edge": 1, "size": 54}, "46": {"level": 3, "pid": 24, "parent_edge": 1, "size": 13}, "47": {"level": 3, "pid": 25, "parent_edge": 3, "size": 203}, "48": {"level": 3, "pid": 26, "parent_edge": 3, "size": 47}, "49": {"level": 3, "pid": 27, "parent_edge": 4, "size": 186}, "50": {"level": 3, "pid": 28, "parent_edge": 4, "size": 60}, "51": {"level": 3, "pid": 29, "parent_edge": 5, "size": 56}, "52": {"level": 3, "pid": 30, "parent_edge": 5, "size": 10}, "53": {"level": 3, "pid": 31, "parent_edge": 5, "size": 95}, "54": {"level": 3, "pid": 32, "parent_edge": 5, "size": 45}, "55": {"level": 3, "pid": 33, "parent_edge": 5, "size": 158}, "56": {"level": 3, "pid": 34, "parent_edge": 5, "size": 82}, "57": {"level": 3, "pid": 35, "parent_edge": 5, "size": 78}, "58": {"level": 3, "pid": 36, "parent_edge": 7, "size": 113}, "59": {"level": 3, "pid": 37, "parent_edge": 8, "size": 32}, "60": {"level": 3, "pid": 38, "parent_edge": 8, "size": 11}, "61": {"level": 3, "pid": 39, "parent_edge": 8, "size": 5}, "62": {"level": 3, "pid": 40, "parent_edge": 8, "size": 19}, "63": {"level": 3, "pid": 41, "parent_edge": 8, "size": 19}, "64": {"level": 3, "pid": 42, "parent_edge": 8, "size": 68}, "65": {"level": 3, "pid": 43, "parent_edge": 8, "size": 135}, "66": {"level": 3, "pid": 44, "parent_edge": 8, "size": 11}, "67": {"level": 3, "pid": 45, "parent_edge": 8, "size": 4}, "68": {"level": 3, "pid": 46, "parent_edge": 8, "size": 4}, "69": {"level": 3, "pid": 47, "parent_edge": 8, "size": 2}, "70": {"level": 3, "pid": 48, "parent_edge": 8, "size": 9}, "71": {"level": 3, "pid": 49, "parent_edge": 8, "size": 2}, "72": {"level": 3, "pid": 50, "parent_edge": 8, "size": 75}, "73": {"level": 3, "pid": 51, "parent_edge": 8, "size": 45}, "74": {"level": 3, "pid": 52, "parent_edge": 8, "size": 3}, "75": {"level": 3, "pid": 53, "parent_edge": 8, "size": 45}, "76": {"level": 3, "pid": 54, "parent_edge": 8, "size": 3}, "77": {"level": 3, "pid": 55, "parent_edge": 8, "size": 5}, "78": {"level": 3, "pid": 56, "parent_edge": 8, "size": 2}, "79": {"level": 3, "pid": 57, "parent_edge": 8, "size": 29}, "80": {"level": 3, "pid": 58, "parent_edge": 8, "size": 15}, "81": {"level": 3, "pid": 59, "parent_edge": 8, "size": 3}, "82": {"level": 3, "pid": 60, "parent_edge": 8, "size": 15}, "83": {"level": 3, "pid": 61, "parent_edge": 8, "size": 3}, "84": {"level": 3, "pid": 62, "parent_edge": 8, "size": 3}, "85": {"level": 3, "pid": 63, "parent_edge": 8, "size": 3}, "86": {"level": 3, "pid": 64, "parent_edge": 8, "size": 41}, "87": {"level": 3, "pid": 65, "parent_edge": 8, "size": 13}, "88": {"level": 3, "pid": 66, "parent_edge": 8, "size": 5}, "89": {"level": 3, "pid": 67, "parent_edge": 8, "size": 26}, "90": {"level": 3, "pid": 68, "parent_edge": 8, "size": 13}, "91": {"level": 3, "pid": 69, "parent_edge": 8, "size": 10}, "92": {"level": 3, "pid": 70, "parent_edge": 8, "size": 109}, "93": {"level": 3, "pid": 71, "parent_edge": 10, "size": 65}, "94": {"level": 3, "pid": 72, "parent_edge": 10, "size": 11}, "95": {"level": 3, "pid": 73, "parent_edge": 10, "size": 2}, "96": {"level": 3, "pid": 74, "parent_edge": 10, "size": 5}, "97": {"level": 3, "pid": 75, "parent_edge": 10, "size": 86}, "98": {"level": 3, "pid": 76, "parent_edge": 11, "size": 60}, "99": {"level": 3, "pid": 77, "parent_edge": 11, "size": 29}, "100": {"level": 3, "pid": 78, "parent_edge": 11, "size": 8}, "101": {"level": 3, "pid": 79, "parent_edge": 11, "size": 6}, "102": {"level": 3, "pid": 80, "parent_edge": 11, "size": 14}, "103": {"level": 3, "pid": 81, "parent_edge": 12, "size": 23}, "104": {"level": 3, "pid": 82, "parent_edge": 12, "size": 177}, "105": {"level": 3, "pid": 83, "parent_edge": 13, "size": 121}, "106": {"level": 3, "pid": 84, "parent_edge": 13, "size": 89}, "107": {"level": 3, "pid": 85, "parent_edge": 13, "size": 53}, "108": {"level": 3, "pid": 86, "parent_edge": 13, "size": 35}, "109": {"level": 3, "pid": 87, "parent_edge": 13, "size": 60}, "110": {"level": 3, "pid": 88, "parent_edge": 13, "size": 9}, "111": {"level": 3, "pid": 89, "parent_edge": 13, "size": 10}, "112": {"level": 3, "pid": 90, "parent_edge": 13, "size": 72}, "113": {"level": 3, "pid": 91, "parent_edge": 13, "size": 22}, "114": {"level": 3, "pid": 92, "parent_edge": 13, "size": 11}, "115": {"level": 3, "pid": 93, "parent_edge": 13, "size": 5}, "116": {"level": 3, "pid": 94, "parent_edge": 13, "size": 44}, "117": {"level": 3, "pid": 95, "parent_edge": 13, "size": 60}, "118": {"level": 3, "pid": 96, "parent_edge": 13, "size": 51}, "119": {"level": 3, "pid": 97, "parent_edge": 13, "size": 27}, "120": {"level": 3, "pid": 98, "parent_edge": 13, "size": 27}, "121": {"level": 3, "pid": 99, "parent_edge": 13, "size": 10}, "122": {"level": 3, "pid": 100, "parent_edge": 13, "size": 13}, "123": {"level": 3, "pid": 101, "parent_edge": 13, "size": 2}, "124": {"level": 3, "pid": 102, "parent_edge": 13, "size": 24}, "125": {"level": 3, "pid": 103, "parent_edge": 13, "size": 2}, "126": {"level": 3, "pid": 104, "parent_edge": 13, "size": 40}, "127": {"level": 3, "pid": 105, "parent_edge": 13, "size": 13}, "128": {"level": 3, "pid": 106, "parent_edge": 13, "size": 10}, "129": {"level": 3, "pid": 107, "parent_edge": 13, "size": 2}, "130": {"level": 3, "pid": 108, "parent_edge": 13, "size": 67}, "131": {"level": 3, "pid": 109, "parent_edge": 13, "size": 2}, "132": {"level": 3, "pid": 110, "parent_edge": 13, "size": 66}, "133": {"level": 3, "pid": 111, "parent_edge": 13, "size": 11}, "134": {"level": 3, "pid": 112, "parent_edge": 13, "size": 16}, "135": {"level": 3, "pid": 113, "parent_edge": 13, "size": 2}, "136": {"level": 3, "pid": 114, "parent_edge": 13, "size": 64}, "137": {"level": 3, "pid": 115, "parent_edge": 13, "size": 2}, "138": {"level": 3, "pid": 116, "parent_edge": 13, "size": 52}, "139": {"level": 3, "pid": 117, "parent_edge": 13, "size": 2}, "140": {"level": 3, "pid": 118, "parent_edge": 13, "size": 45}, "141": {"level": 3, "pid": 119, "parent_edge": 13, "size": 2}, "142": {"level": 3, "pid": 120, "parent_edge": 13, "size": 73}, "143": {"level": 3, "pid": 121, "parent_edge": 13, "size": 50}, "144": {"level": 3, "pid": 122, "parent_edge": 13, "size": 78}, "145": {"level": 3, "pid": 123, "parent_edge": 13, "size": 11}, "146": {"level": 3, "pid": 124, "parent_edge": 13, "size": 12}, "147": {"level": 3, "pid": 125, "parent_edge": 13, "size": 2}, "148": {"level": 3, "pid": 126, "parent_edge": 13, "size": 68}, "149": {"level": 3, "pid": 127, "parent_edge": 13, "size": 2}, "150": {"level": 3, "pid": 128, "parent_edge": 13, "size": 66}, "151": {"level": 3, "pid": 129, "parent_edge": 13, "size": 10}, "152": {"level": 3, "pid": 130, "parent_edge": 13, "size": 3}, "153": {"level": 3, "pid": 131, "parent_edge": 13, "size": 177}, "154": {"level": 3, "pid": 132, "parent_edge": 15, "size": 128}, "155": {"level": 3, "pid": 133, "parent_edge": 16, "size": 53}, "156": {"level": 3, "pid": 134, "parent_edge": 16, "size": 11}, "157": {"level": 3, "pid": 135, "parent_edge": 16, "size": 5}, "158": {"level": 3, "pid": 136, "parent_edge": 16, "size": 19}, "159": {"level": 3, "pid": 137, "parent_edge": 17, "size": 133}, "160": {"level": 3, "pid": 138, "parent_edge": 18, "size": 80}, "161": {"level": 3, "pid": 139, "parent_edge": 18, "size": 125}, "162": {"level": 3, "pid": 140, "parent_edge": 18, "size": 81}, "163": {"level": 3, "pid": 141, "parent_edge": 19, "size": 43}, "164": {"level": 3, "pid": 142, "parent_edge": 19, "size": 67}, "165": {"level": 3, "pid": 143, "parent_edge": 19, "size": 55}, "166": {"level": 3, "pid": 144, "parent_edge": 19, "size": 26}, "167": {"level": 3, "pid": 145, "parent_edge": 19, "size": 90}, "168": {"level": 3, "pid": 146, "parent_edge": 20, "size": 28}, "169": {"level": 3, "pid": 147, "parent_edge": 21, "size": 51}, "170": {"level": 3, "pid": 148, "parent_edge": 21, "size": 36}, "171": {"level": 3, "pid": 149, "parent_edge": 21, "size": 13}, "172": {"level": 3, "pid": 150, "parent_edge": 21, "size": 10}, "173": {"level": 3, "pid": 151, "parent_edge": 21, "size": 11}, "174": {"level": 3, "pid": 152, "parent_edge": 21, "size": 16}, "175": {"level": 3, "pid": 153, "parent_edge": 21, "size": 61}, "176": {"level": 3, "pid": 154, "parent_edge": 21, "size": 115}, "177": {"level": 3, "pid": 155, "parent_edge": 21, "size": 55}, "178": {"level": 3, "pid": 156, "parent_edge": 21, "size": 44}, "179": {"level": 3, "pid": 157, "parent_edge": 21, "size": 40}, "180": {"level": 3, "pid": 158, "parent_edge": 21, "size": 55}, "181": {"level": 3, "pid": 159, "parent_edge": 21, "size": 43}, "182": {"level": 3, "pid": 160, "parent_edge": 21, "size": 76}, "183": {"level": 3, "pid": 161, "parent_edge": 21, "size": 125}, "184": {"level": 3, "pid": 162, "parent_edge": 21, "size": 19}, "185": {"level": 3, "pid": 163, "parent_edge": 21, "size": 13}, "186": {"level": 3, "pid": 164, "parent_edge": 21, "size": 136}, "187": {"level": 3, "pid": 165, "parent_edge": 21, "size": 66}, "188": {"level": 3, "pid": 166, "parent_edge": 21, "size": 70}, "189": {"level": 3, "pid": 167, "parent_edge": 21, "size": 42}, "190": {"level": 3, "pid": 168, "parent_edge": 21, "size": 61}, "191": {"level": 3, "pid": 169, "parent_edge": 21, "size": 48}, "192": {"level": 3, "pid": 170, "parent_edge": 21, "size": 29}, "193": {"level": 3, "pid": 171, "parent_edge": 21, "size": 1}, "194": {"content": "Social Activity Measurement by Counting Faces Captured in\nFirst-Person View Lifelogging Video", "parent_edge": 22, "level": 4, "size": 12}, "195": {"content": "Akane Okuno\nFuture University Hakodate\nHokkaido, Japan\na-okuno@sumilab.org", "parent_edge": 23, "level": 4, "size": 11}, "196": {"content": "Yasuyuki Sumi\nFuture University Hakodate\nHokkaido, Japan\nsumi@acm.org", "parent_edge": 24, "level": 4, "size": 11}, "197": {"content": "Figure 1: Examples of the scene captured using first-person view lifelogging video.", "parent_edge": 25, "level": 4, "size": 14}, "198": {"content": "ABSTRACT\nThis paper proposes a method to measure the daily face-to-face\nsocial activity of a camera wearer by detecting faces captured in\nfirst-person view lifelogging videos.", "parent_edge": 26, "level": 4, "size": 27}, "199": {"content": "This study was inspired by\npedometers used to estimate the amount of physical activity by\ncounting the number of steps detected by accelerometers, which is\neffective for reflecting individual health and facilitating behavior\nchange.", "parent_edge": 26, "level": 4, "size": 36}, "200": {"content": "We investigated whether we can estimate the amount of\nsocial activity by counting the number of faces captured in the first-\nperson view videos like a pedometer.", "parent_edge": 26, "level": 4, "size": 28}, "201": {"content": "Our system counts not only\nthe number of faces but also weighs in the numbers according to the\nsize of the face (corresponding to a face\u2019s closeness) and the amount\nof time it was shown in the video.", "parent_edge": 26, "level": 4, "size": 43}, "202": {"content": "By doing so, we confirmed that\nwe can measure the amount of social activity based on the quality\nof each interaction.", "parent_edge": 26, "level": 4, "size": 23}, "203": {"content": "For example, if we simply count the number\nof faces, we overestimate social activities while passing through\na crowd of people.", "parent_edge": 26, "level": 4, "size": 24}, "204": {"content": "Our system, on the other hand, gives a higher\nscore to a social actitivity even when speaking with a single person\nfor a long time, which was also positively evaluated by experiment\nparticipants who viewed the lifelogging videos.", "parent_edge": 26, "level": 4, "size": 42}, "205": {"content": "Through evaluation\nexperiments, many evaluators evaluated the social activity high\nwhen the camera wearer speaks.", "parent_edge": 26, "level": 4, "size": 17}, "206": {"content": "An interesting feature of the pro-\nposed system is that it can correctly evaluate such scenes higher as\nthe camera wearer actively engages in conversations with others,\neven though the system does not measure the camera wearer\u2019s\nutterances.", "parent_edge": 26, "level": 4, "size": 42}, "207": {"content": "This is because the conversation partners tend to turn\ntheir faces towards to the camera wearer, and that increases the", "parent_edge": 26, "level": 4, "size": 21}, "208": {"content": "Permission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page.", "parent_edge": 27, "level": 4, "size": 51}, "209": {"content": "Copyrights for components of this work owned by others than the\nauthor(s) must be honored.", "parent_edge": 27, "level": 4, "size": 19}, "210": {"content": "Abstracting with credit is permitted.", "parent_edge": 27, "level": 4, "size": 6}, "211": {"content": "To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee.", "parent_edge": 27, "level": 4, "size": 25}, "212": {"content": "Request permissions from permissions@acm.org.", "parent_edge": 27, "level": 4, "size": 7}, "213": {"content": "AH2019, March 11\u201312, 2019, Reims, France\n\u00a9 2019 Copyright held by the owner/author(s).", "parent_edge": 27, "level": 4, "size": 21}, "214": {"content": "Publication rights licensed to ACM.", "parent_edge": 27, "level": 4, "size": 6}, "215": {"content": "ACM ISBN 978-1-4503-6547-5/19/03.", "parent_edge": 27, "level": 4, "size": 4}, "216": {"content": "https://doi.org/10.1145/3311823.3311846", "parent_edge": 27, "level": 4, "size": 3}, "217": {"content": "number of detected faces as a result.", "parent_edge": 28, "level": 4, "size": 8}, "218": {"content": "However, the present system\nfails to correctly estimate the depth of social activity compared to\nwhat the camera wearer recalls especially when the conversation\npartners are standing out of the camera\u2019s field of view.", "parent_edge": 28, "level": 4, "size": 38}, "219": {"content": "The paper\nbriefly descibes how the results can be improved by widening the\ncamera\u2019s field of view.", "parent_edge": 28, "level": 4, "size": 20}, "220": {"content": "CCS CONCEPTS\n\u2022 Human-centered computing \u2192 Human computer interaction\n(HCI); Ubiquitous and mobile computing; Visualization; Empirical\nstudies in HCI.", "parent_edge": 29, "level": 4, "size": 25}, "221": {"content": "KEYWORDS\nSocial activity measurement, first-person view video, lifelogging,\nface detection, quantified self, social health", "parent_edge": 30, "level": 4, "size": 19}, "222": {"content": "ACM Reference Format:\nAkane Okuno and Yasuyuki Sumi.", "parent_edge": 31, "level": 4, "size": 10}, "223": {"content": "2019.", "parent_edge": 31, "level": 4, "size": 2}, "224": {"content": "Social Activity Measurement by\nCounting Faces Captured in First-Person View Lifelogging Video.", "parent_edge": 31, "level": 4, "size": 13}, "225": {"content": "In Aug-\nmented Human International Conference 2019 (AH2019), March 11\u201312, 2019,\nReims, France.", "parent_edge": 31, "level": 4, "size": 20}, "226": {"content": "ACM, New York, NY, USA, 9 pages.", "parent_edge": 31, "level": 4, "size": 12}, "227": {"content": "https://doi.org/10.1145/\n3311823.3311846", "parent_edge": 31, "level": 4, "size": 4}, "228": {"content": "1 INTRODUCTION\nThe question of this research is, \"Is it possible to measure the face-\nto-face engagement level, that is, the amount of daily social activity,\nwith a simple method?\"", "parent_edge": 32, "level": 4, "size": 37}, "229": {"content": "This paper proposes a method to measure\nthe social activity of engagement with other people by counting\nfaces captured in a lifelogging video (Figure 1).", "parent_edge": 32, "level": 4, "size": 28}, "230": {"content": "Originally, the pedometer was an instrument to count steps.", "parent_edge": 33, "level": 4, "size": 11}, "231": {"content": "In\nrecent years, as a result of advances in technology to recognition\npatterns of fluctuations in body motion, many wristband-type ac-\ntivity meters (Fitbit, Jawbone, etc.)", "parent_edge": 33, "level": 4, "size": 33}, "232": {"content": "are able to identify walking,\njogging, sleeping, etc.", "parent_edge": 33, "level": 4, "size": 12}, "233": {"content": "[7].", "parent_edge": 33, "level": 4, "size": 4}, "234": {"content": "By aggregating and comparing the data\nof tens of thousands of users, the objective vision of individual ex-\nercise and sleep quantity is simplified, which promotes motivation\nfor exercise.", "parent_edge": 33, "level": 4, "size": 32}, "235": {"content": "1\fAH2019, March 11\u201312, 2019, Reims, France", "parent_edge": 34, "level": 4, "size": 11}, "236": {"content": "Akane Okuno et al.", "parent_edge": 35, "level": 4, "size": 5}, "237": {"content": "Figure 2: Social activity measurement by counting faces captured in first-person view lifelogging video", "parent_edge": 36, "level": 4, "size": 15}, "238": {"content": "First of all, we count the number of faces.", "parent_edge": 37, "level": 4, "size": 11}, "239": {"content": "We aim to realize a\nface-meter that keeps track of changes in face-to-face engagement\nbased on the time pattern and that records daily social activity.", "parent_edge": 37, "level": 4, "size": 26}, "240": {"content": "This is an analogy with the pedometer that accumulates changes\nin acceleration during exercise and records daily physical activity.", "parent_edge": 37, "level": 4, "size": 20}, "241": {"content": "We quantify face-to-face engagement with people and integrate\nover time.", "parent_edge": 37, "level": 4, "size": 11}, "242": {"content": "This will make it possible to visualize and review the level\nof engagement in daily face-to-face social activity, of which it is\ndifficult to be aware.", "parent_edge": 37, "level": 4, "size": 28}, "243": {"content": "In this study, the value obtained by integrating\nthe face-to-face engagement level over time is defined as the amount\nof social activity, which we measure.", "parent_edge": 37, "level": 4, "size": 28}, "244": {"content": "By wearing a camera and\nacting, various scenes are captured, as shown in Figure 1.", "parent_edge": 37, "level": 4, "size": 18}, "245": {"content": "Face-to-\nface communication occurs often in daily life, such as when we\nmeet and talk to people.", "parent_edge": 37, "level": 4, "size": 19}, "246": {"content": "We use the first-person view video as the\nlifelogging sensor device (first-person view lifelogging video).", "parent_edge": 37, "level": 4, "size": 18}, "247": {"content": "We propose a method to measure the daily social activity of a\ncamera wearer by detecting the face captured in the first-person\nview lifelogging video.", "parent_edge": 38, "level": 4, "size": 26}, "248": {"content": "This is done to quantify the face-to-face\nengagement level using a simple method (Figure 2).", "parent_edge": 38, "level": 4, "size": 18}, "249": {"content": "To record a\nstable image, we attach the camera to the chest.", "parent_edge": 38, "level": 4, "size": 14}, "250": {"content": "In this research, to\ndiscuss the requirements of system realization (camera view angle,\nevaluation formula) and subjective evaluation of users, we record\nvideo as part of the research process.", "parent_edge": 38, "level": 4, "size": 35}, "251": {"content": "The recorded data are used\nafter the camera wearer has confirmed whether there are data to be\nconsidered.", "parent_edge": 38, "level": 4, "size": 19}, "252": {"content": "When this system is used outside the experiment, only\nthe image-processing results and numerical values will be recorded.", "parent_edge": 38, "level": 4, "size": 20}, "253": {"content": "Users will look back on their own social activity on a smartphone\nor similar device.", "parent_edge": 38, "level": 4, "size": 16}, "254": {"content": "We also expect the camera to be small enough to\nallow for natural sensing.", "parent_edge": 38, "level": 4, "size": 15}, "255": {"content": "In the future, we will aim to provide feedback that leads to be-\nhavioral changes to improve social health [9], such as loneliness\nand fatigue in social activities.", "parent_edge": 39, "level": 4, "size": 33}, "256": {"content": "By counting according to the situa-\ntion, users can set the target amount of social activity when using\nthe system in daily life.", "parent_edge": 39, "level": 4, "size": 25}, "257": {"content": "Figure 3 is our prototype application.", "parent_edge": 39, "level": 4, "size": 7}, "258": {"content": "This\ntaxonomy allows users to ascertain whether they tend to pass by\npeople or spend time with them, and how many people they interact\nwith.", "parent_edge": 39, "level": 4, "size": 27}, "259": {"content": "In addition, by recording and recalling daily social activity, we\nthink that users might be able to perceive that there were too few\nor too many face-to-face engagements with people.", "parent_edge": 39, "level": 4, "size": 33}, "260": {"content": "The prototype\nUI is based on an activity gauge, like that of the Apple Watch, and\nan animation of reviewing the activity, like that of SmartBand.", "parent_edge": 39, "level": 4, "size": 30}, "261": {"content": "We examined the case in which the amount of social activity\ntends to feel large by subjective evaluation experiment.", "parent_edge": 40, "level": 4, "size": 20}, "262": {"content": "In this", "parent_edge": 40, "level": 4, "size": 2}, "263": {"content": "Figure 3: Amount of face-to-face social activity can be visual-\nized similarly to the amount of physical activity on a smart\nphone.", "parent_edge": 41, "level": 4, "size": 24}, "264": {"content": "paper, we discuss the effectiveness and limitation of quantifying\nface-to-face engagement level based on face detection in terms\nof the inclusiveness of multiple active behavioral characteristics\nwithout detailed sensing.", "parent_edge": 42, "level": 4, "size": 31}, "265": {"content": "Finally, we improved the measurement of\ndialogue at diagonal or side-by-side positions by using a hemispher-\nical camera.", "parent_edge": 42, "level": 4, "size": 20}, "266": {"content": "The conclusions obtained from subjective evaluation\nexperiments are shown below.", "parent_edge": 42, "level": 4, "size": 11}, "267": {"content": "\u2022 In the situation in which the user spoke and/or engaged,\nthe amount of social activity was evaluated as high, and\nwe confirmed the tendency of the other person facing the\ncamera wearer.", "parent_edge": 43, "level": 4, "size": 36}, "268": {"content": "In other words, it was suggested that social\nactivity can be measured by detecting the face of the part-\nner when the camera wearer performs an active behavior,\nwithout measuring the utterance or the gesture itself.", "parent_edge": 43, "level": 4, "size": 39}, "269": {"content": "\u2022 It was shown that there were scenes that were different from\nthe subjective evaluation when the amount of social activity\nwas calculated by only counting the number of faces.", "parent_edge": 44, "level": 4, "size": 31}, "270": {"content": "To\nquantify the amount of social activity, considering the active\nbehavior, it is necessary to weight by proximity and time\ncontinuity.", "parent_edge": 44, "level": 4, "size": 24}, "271": {"content": "\u2022 A few diagonal and side-by-side dialogues were found, and\nit was determined that an angle of view of 180\u25e6 or more was\nnecessary.", "parent_edge": 45, "level": 4, "size": 26}, "272": {"content": "As a result of using a 200\u25e6 hemispherical camera,\nit was suggested that measurement of situations with a high\nlevel of face-to-face engagement can be improved.", "parent_edge": 45, "level": 4, "size": 28}, "273": {"content": "2\fSocial Activity Measurement by Counting Faces Captured in\nFirst-Person View Lifelogging Video", "parent_edge": 46, "level": 4, "size": 13}, "274": {"content": "2 RELATED WORK\n2.1 Recognition of Social Contexts\nTechniques for recognizing the social context of individuals and\ngroups from nonvisual information have been studied by many\nresearchers.", "parent_edge": 47, "level": 4, "size": 28}, "275": {"content": "For example, by combining exercise with an accel-\neration sensor [17], a voice with a speaker [16], distance with a\nBluetooth link [5], and face-to-face detection with an IR sensor\n[3], various aspects of social context have been measured.", "parent_edge": 47, "level": 4, "size": 53}, "276": {"content": "The re-\nsults include predictions about productivity and job satisfaction\n[15].", "parent_edge": 47, "level": 4, "size": 14}, "277": {"content": "Alternatively, technologies that interpret a talking field [14]\nenabled researchers to use a simple algorithm and a lightweight,\nnetworked mobile terminal equipped with a microphone to work\nin crowds.", "parent_edge": 47, "level": 4, "size": 34}, "278": {"content": "There are also studies that recognized social context\nby sensing the wearers themselves.", "parent_edge": 47, "level": 4, "size": 14}, "279": {"content": "For example, a technique of\nincorporating a photo reflector into eyeglasses and measuring fa-\ncial expressions from skin deformation [13] enabled researchers to\nrecord complex, daily facial expressions in lifelogs using machine\nlearning.", "parent_edge": 47, "level": 4, "size": 38}, "280": {"content": "Moreover, social context recognition has been studied\nusing a large amount of long-term data from virtual space [19].", "parent_edge": 47, "level": 4, "size": 22}, "281": {"content": "There are many aspects of social activities that require us to\nwell-recognize important information, according to the purpose\nand scope of its application.", "parent_edge": 48, "level": 4, "size": 25}, "282": {"content": "We assess the face-to-face social context\nof a camera wearer using faces that are cues from the first-person\nview lifelogging video.", "parent_edge": 48, "level": 4, "size": 22}, "283": {"content": "2.2 Technique with First-Person View Video\nMany techniques have been studied for analyzing first-person view\nvideo.", "parent_edge": 49, "level": 4, "size": 17}, "284": {"content": "There have also been studies on techniques for recognizing\nsocial contexts.", "parent_edge": 49, "level": 4, "size": 12}, "285": {"content": "For example, calculating the line of sight of an-\nother person\u2019s face from the position and orientation of the camera\nwearer allows software to create a 3D mapping of the environ-\nment, thus creating a heat map.", "parent_edge": 49, "level": 4, "size": 42}, "286": {"content": "This can be used to estimate the\npartner\u2019s profile and role in a group setting [6].", "parent_edge": 49, "level": 4, "size": 21}, "287": {"content": "A few studies rec-\nognize egocentric social situations by measuring the behavior of\nthe camera wearer using the first-person view video[21].", "parent_edge": 49, "level": 4, "size": 25}, "288": {"content": "Addition-\nally, multiple camera wearers\u2019 scenes can be analyzed to correlate\nhead movement and faces during a group conversation.", "parent_edge": 49, "level": 4, "size": 22}, "289": {"content": "Thus, it\nis also possible to derive the position and orientation of the face\nof the camera wearer [23].", "parent_edge": 49, "level": 4, "size": 23}, "290": {"content": "There is another technique that uses\nthe affinity of head direction to assess the social factors in a group\nconversation [1].", "parent_edge": 49, "level": 4, "size": 24}, "291": {"content": "It is useful to understand the social context of the camera wearer\nfrom first-person view video.", "parent_edge": 50, "level": 4, "size": 17}, "292": {"content": "With our approach, we measure the\nlevel of engagement with others by calculating the number of people\nwith detected frontal faces, distances, and the time-weight of the\ncontinuity to quantify the daily face-to-face social activity with a\nsimple method.", "parent_edge": 50, "level": 4, "size": 43}, "293": {"content": "2.3 Utilization of Captured Experiential Data\nResearch that extends meta-recognition by combining egocentric\nand objective information using first-person view video and/or\nmetadata has been studied.", "parent_edge": 51, "level": 4, "size": 26}, "294": {"content": "For example, there is research that\nsupports memory recall that is difficult for people with memory im-\npairment [8] and research that supports control of everyday eating", "parent_edge": 51, "level": 4, "size": 30}, "295": {"content": "AH2019, March 11\u201312, 2019, Reims, France", "parent_edge": 52, "level": 4, "size": 10}, "296": {"content": "habits [18].", "parent_edge": 53, "level": 4, "size": 5}, "297": {"content": "Additionally, the influence of egocentric and/or objec-\ntive information on metacognition has been studied empirically.", "parent_edge": 53, "level": 4, "size": 17}, "298": {"content": "For\nexample, although vision information promotes recall of detailed\nmemory, location information is reported to support inferential\nprocesses [11].", "parent_edge": 53, "level": 4, "size": 24}, "299": {"content": "Furthermore, because metacognition perceptually\nchanges over time, information on meta-viewpoints is reported to\nbe useful for reviewing experiences [20].", "parent_edge": 53, "level": 4, "size": 24}, "300": {"content": "On the other hand, there\nis research to expand self-perception by using the first-person view\nvideo of others in parallel [12].", "parent_edge": 53, "level": 4, "size": 25}, "301": {"content": "We quantify face-to-face engagement with people using a first-\nperson view lifelogging video.", "parent_edge": 54, "level": 4, "size": 14}, "302": {"content": "Our motivation is that this will enable\nusers to visualize and reflect on the level of engagement in daily\nface-to-face social activity that it is difficult to be aware of.", "parent_edge": 54, "level": 4, "size": 31}, "303": {"content": "3 SOCIAL ACTIVITY MEASUREMENT\nIn this study, the value obtained by integrating the face-to-face\nengagement level over time is defined as the amount of social\nactivity, which we measure.", "parent_edge": 55, "level": 4, "size": 32}, "304": {"content": "We show the example result (Figure 4)\nobtained from social activity measurement using our proposed\nmethod compared with a result using a method that only counts\nthe number of faces.", "parent_edge": 55, "level": 4, "size": 33}, "305": {"content": "If we only count the number of faces, we treat\nencounters with other people in crowds and close dialogue with\nspecific persons in the same way; thus, we propose counting them\nseparately with distance and time continuity.", "parent_edge": 55, "level": 4, "size": 41}, "306": {"content": "The amount of social\nactivity shall be the time integral of the value calculated on the basis\nof the number of people, the proximity, and continuity for each\nframe.", "parent_edge": 55, "level": 4, "size": 32}, "307": {"content": "The proposed method and method of counting the number\nof faces measured the amount of social activity every second.", "parent_edge": 55, "level": 4, "size": 20}, "308": {"content": "For example, the amount of social activity is calculated for a\nsituation in which one person talks to a specific person at a short\ndistance (Figure 4: S1).", "parent_edge": 56, "level": 4, "size": 33}, "309": {"content": "Additionally, the amount of social activity\nis calculated in consideration of the situation where three people\nare talking while maintaining the distance (S2).", "parent_edge": 56, "level": 4, "size": 27}, "310": {"content": "Furthermore, the\namount of social activity is calculated considering the instantaneous\ninvolvement with people in the crowd (S3).", "parent_edge": 56, "level": 4, "size": 22}, "311": {"content": "With the method of counting faces, the cumulative amount of\nsocial activity for approximately 20 s is in the order of S1 < S2 \u2248 S3,\nbut with the proposed method, it is S3 < S2 \u2248 S1.", "parent_edge": 57, "level": 4, "size": 42}, "312": {"content": "Additionally,\nour method is calculated considering scenes in which there are\nsituations when conversation partner does or does not keep facing\nthe camera wearer, as in frame t + 9 of Figure 4.", "parent_edge": 57, "level": 4, "size": 36}, "313": {"content": "3.1 Requirements\nFor face detection, we use OpenFace developed by Carnegie Mellon\nUniversity [2].", "parent_edge": 58, "level": 4, "size": 18}, "314": {"content": "Face tracking of the dlib library [4] in OpenFace\ntracks what was estimated as the same person\u2019s face between frames\n(Figure 5).", "parent_edge": 58, "level": 4, "size": 29}, "315": {"content": "In the case of measurement every 10 s, the time continuity\nTi (t ) increases by 1.", "parent_edge": 58, "level": 4, "size": 20}, "316": {"content": "We do not detect the sideways face and occipital\narea but detect frontal faces and measure face-to-face engagement.", "parent_edge": 58, "level": 4, "size": 19}, "317": {"content": "In terms of design simplicity and privacy consideration, we think\nit is useful to use the face-detection result without identifying the\nindividual\u2019s face.", "parent_edge": 58, "level": 4, "size": 27}, "318": {"content": "3.2 Implementation\nSpecifically, the amount of social activity S of a certain time t is\ncalculated by Formula (1).", "parent_edge": 59, "level": 4, "size": 23}, "319": {"content": "The closeness Di (t ) represents the size", "parent_edge": 59, "level": 4, "size": 9}, "320": {"content": "3\fAH2019, March 11\u201312, 2019, Reims, France", "parent_edge": 60, "level": 4, "size": 11}, "321": {"content": "Akane Okuno et al.", "parent_edge": 61, "level": 4, "size": 5}, "322": {"content": "Figure 4: Examples of measurement: Dialogue with an individual, Multiparty conversation, and passing with people", "parent_edge": 62, "level": 4, "size": 19}, "323": {"content": "of the face occupying the entire screen.", "parent_edge": 63, "level": 4, "size": 8}, "324": {"content": "Specifically, it is calculated\nby Formula (2).", "parent_edge": 63, "level": 4, "size": 11}, "325": {"content": "That is, for each frame, the product of the size of each detected\nface and the continuity at that time is obtained and accumulated.", "parent_edge": 64, "level": 4, "size": 27}, "326": {"content": "By time-integrating this information, for example, it is possible to\nmeasure the amount of social activity during the whole day, extract\na specific scene in time, and evaluate the amount of social activity\nof the scene.", "parent_edge": 64, "level": 4, "size": 41}, "327": {"content": "For the detected face identification (ID) number i, the newly\nissued ID is used every time OpenFace detects a new face.", "parent_edge": 65, "level": 4, "size": 25}, "328": {"content": "Specifi-\ncally, different IDs are issued for newly detected faces in a certain\nframe.", "parent_edge": 65, "level": 4, "size": 16}, "329": {"content": "However, the same ID is given to the face determined to be\nthe same person as the face detected in the immediately preceding\nframe.", "parent_edge": 65, "level": 4, "size": 26}, "330": {"content": "However, when two or more undetected frames intervene,\nanother new ID is issued, even for the same person\u2019s face.", "parent_edge": 65, "level": 4, "size": 25}, "331": {"content": "Utilizing\nthis property, we decided to increment Ti (t ) for that ID if the same\nID is detected in consecutive frames, and we use this value as time\ncontinuity.", "parent_edge": 65, "level": 4, "size": 34}, "332": {"content": "Ti (t ) always starts from 1.", "parent_edge": 65, "level": 4, "size": 9}, "333": {"content": "Figure 5: Calculation of size and continuity of each face", "parent_edge": 66, "level": 4, "size": 11}, "334": {"content": "m(cid:88)", "parent_edge": 67, "level": 4, "size": 4}, "335": {"content": "n(cid:88)", "parent_edge": 68, "level": 4, "size": 4}, "336": {"content": "S =", "parent_edge": 69, "level": 4, "size": 2}, "337": {"content": "Ti (t ) \u00b7 Di (t )", "parent_edge": 70, "level": 4, "size": 9}, "338": {"content": "t =1", "parent_edge": 71, "level": 4, "size": 2}, "339": {"content": "i=1\ni : The identification number of the detected face,\nTi (t ) : Continuity (same face-detection frame),\nDi (t ) : Closeness (the area of the face occupying the\nwhole),\nm : The number of measurement frames (elapsed ti-\nme) up to time t,\nn : The cumulative number of people (number of fa-\nces) up to time t.", "parent_edge": 72, "level": 4, "size": 75}, "340": {"content": "(cid:42)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)", "parent_edge": 73, "level": 4, "size": 45}, "341": {"content": "(cid:44)", "parent_edge": 74, "level": 4, "size": 3}, "342": {"content": "(cid:43)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)", "parent_edge": 75, "level": 4, "size": 45}, "343": {"content": "(cid:45)", "parent_edge": 76, "level": 4, "size": 3}, "344": {"content": "Di = wi \u00b7 hi", "parent_edge": 77, "level": 4, "size": 5}, "345": {"content": "\u00b7 100", "parent_edge": 78, "level": 4, "size": 2}, "346": {"content": "R\nwi : The width of the detected face,\nhi : The height of the detected face,\nR : Screen resolution.", "parent_edge": 79, "level": 4, "size": 24}, "347": {"content": "The units are pixels.", "parent_edge": 79, "level": 4, "size": 5}, "348": {"content": "(cid:42)(cid:46)(cid:46)(cid:46)(cid:46)", "parent_edge": 80, "level": 4, "size": 15}, "349": {"content": "(cid:44)", "parent_edge": 81, "level": 4, "size": 3}, "350": {"content": "(cid:43)(cid:47)(cid:47)(cid:47)(cid:47)", "parent_edge": 82, "level": 4, "size": 15}, "351": {"content": "(cid:45)", "parent_edge": 83, "level": 4, "size": 3}, "352": {"content": "(1)", "parent_edge": 84, "level": 4, "size": 3}, "353": {"content": "(2)", "parent_edge": 85, "level": 4, "size": 3}, "354": {"content": "4 SUBJECTIVE EVALUATION EXPERIMENT\nWe examined the scene in which the user tends to feel that the\namount of social activity is large by subjective evaluation exper-\niment.", "parent_edge": 86, "level": 4, "size": 29}, "355": {"content": "The results of the subjective evaluation experiment will\nreveal two questions.", "parent_edge": 86, "level": 4, "size": 12}, "356": {"content": "\u2022 What kind of scene tends to feel as though the amount of", "parent_edge": 87, "level": 4, "size": 13}, "357": {"content": "social activity is large?", "parent_edge": 88, "level": 4, "size": 5}, "358": {"content": "\u2022 Is it possible to measure the face-to-face engagement level,\nthat is, the daily amount of social activity, with a simple\nmethod?", "parent_edge": 89, "level": 4, "size": 26}, "359": {"content": "4\fSocial Activity Measurement by Counting Faces Captured in\nFirst-Person View Lifelogging Video", "parent_edge": 90, "level": 4, "size": 13}, "360": {"content": "AH2019, March 11\u201312, 2019, Reims, France", "parent_edge": 91, "level": 4, "size": 10}, "361": {"content": "In order to quantitatively evaluate the subjective amount of\nsocial activity, we instructed 8 persons (Table 2) to browse and\nrearrange the various first-person view videos (Table 1).", "parent_edge": 92, "level": 4, "size": 34}, "362": {"content": "Each of\nvideo contents is a uniformly extracted 1-min activity from the\nfirst-person view lifelogging video that is recorded by a visitor who\nparticipated in the demonstration and poster session at a conference.", "parent_edge": 92, "level": 4, "size": 34}, "363": {"content": "Finally, we compared the quantified subjective evaluation using an\nordinal scale that is obtained from 8 evaluators and the amount\nof social activity quantified by the proposed method and by the\nmethod of only counting the number of faces.", "parent_edge": 92, "level": 4, "size": 41}, "364": {"content": "4.1 Data Collection\nFrom the 8 first-person view lifelogging videos recorded during\nthe demonstration and poster session, we chose Person P8\u2019s video\nof the person who had conversations with other participants in\nthe same place.", "parent_edge": 93, "level": 4, "size": 39}, "365": {"content": "We uniformly extracted 10 videos that contain 1\nminute of various social activities from the approximately 1.5-h\nfirst-person view lifelogging video (Table 1).", "parent_edge": 93, "level": 4, "size": 26}, "366": {"content": "Table 1: The extracted 10 videos from the 1.5-h video", "parent_edge": 94, "level": 4, "size": 11}, "367": {"content": "Video contents", "parent_edge": 95, "level": 4, "size": 2}, "368": {"content": "Talking to the Person P1", "parent_edge": 96, "level": 4, "size": 5}, "369": {"content": "A Walking alone in the hallway\nB Walking in the crowd and moving toward the presenter\nC Talking to the presenter and walking in the crowd\nD Talking to the presenter while experiencing the exhibit\nE\nF Group conversation with the Person P1 and another person\nG Hearing a presenter\u2019s talk from afar\nH Encounter with Person P1 and having a short conversation\nHearing a conversation behind the presenter and visitor\nI\nHearing a presenter\u2019s talk from afar with many visitors\nJ", "parent_edge": 97, "level": 4, "size": 86}, "370": {"content": "4.2 Experiment Participant\nThe evaluators were a total of 8 persons, including a camera wearer,\na dialogue partner, and 6 third parties (Table 2).", "parent_edge": 98, "level": 4, "size": 30}, "371": {"content": "Furthermore, the\nsubjective evaluation was compared from the following three view-\npoints.", "parent_edge": 98, "level": 4, "size": 14}, "372": {"content": "This is to consider the influence of the difference in experi-\nence on the evaluation.", "parent_edge": 98, "level": 4, "size": 16}, "373": {"content": "\u2022 Person\u2019s viewpoint as a camera wearer\n\u2022 Person\u2019s viewpoint as a dialogue partner in videos\n\u2022 Third-party viewpoint that did not appear in videos", "parent_edge": 99, "level": 4, "size": 29}, "374": {"content": "Table 2: Participants in subjective evaluation experiment", "parent_edge": 100, "level": 4, "size": 8}, "375": {"content": "Camera wearer\nDialogue partner\nThird person", "parent_edge": 101, "level": 4, "size": 6}, "376": {"content": "Participants\nP8\nP1\nP2,P3,P4,P5,P6,P7", "parent_edge": 102, "level": 4, "size": 14}, "377": {"content": "4.3 Experimental Procedure\nWe uniformly extracted 10 videos that contains 1 minute of vari-\nous social activities from the approximately 1.5-h first-person view", "parent_edge": 103, "level": 4, "size": 23}, "378": {"content": "lifelogging video (Table 1).", "parent_edge": 104, "level": 4, "size": 7}, "379": {"content": "Additionally, the 8 evaluators (Table 2)\nwere instructed to watch all 10 videos and rearrange them using\ntwo symbols, < and =, to sort the videos in ascending order of the\namount of social activity.", "parent_edge": 104, "level": 4, "size": 41}, "380": {"content": "We also instructed them to describe the\nmemo of 0 to 100 and the judgment criteria so as not to mistake the\nsorting order.", "parent_edge": 104, "level": 4, "size": 25}, "381": {"content": "The time required for evaluation was approximately\n20 min.", "parent_edge": 104, "level": 4, "size": 10}, "382": {"content": "These tasks were conducted online by all evaluators.", "parent_edge": 104, "level": 4, "size": 9}, "383": {"content": "Then,\nwe quantified the subjective evaluation of the amount of social ac-\ntivity using an ordinal distance in which the videos were rearranged.", "parent_edge": 104, "level": 4, "size": 25}, "384": {"content": "Finally, we compared the quantified subjective evaluation using an\nordinal scale that was obtained from multiple evaluators with the\namounts of social activity quantified by the proposed method and\nby the method only counting the number of faces.", "parent_edge": 104, "level": 4, "size": 40}, "385": {"content": "The proposed\nmethod and the method counting the number of faces measured\nthe amount of social activity every second.", "parent_edge": 104, "level": 4, "size": 20}, "386": {"content": "4.4 Results of the Experiment\nWe show the experimental results obtained from 10 videos rear-\nranged in ascending order of quantified subjective evaluation in\nFigure 6.", "parent_edge": 105, "level": 4, "size": 27}, "387": {"content": "The results were rearranged in order of median of quanti-\nfied subjective evaluation of the amount of social activity.", "parent_edge": 105, "level": 4, "size": 20}, "388": {"content": "When the\nsame value was obtained, the videos were sorted alphabetically.", "parent_edge": 105, "level": 4, "size": 13}, "389": {"content": "We\nalso plotted the amount of social activity quantified by the proposed\nmethod (PM) and that quantified by counting the number of faces\n(NF).", "parent_edge": 105, "level": 4, "size": 29}, "390": {"content": "Figure 6 shows the order of the obtained results.", "parent_edge": 105, "level": 4, "size": 10}, "391": {"content": "Because each\ndetailed scale is different, the calculated amount of social activity\nis plotted in accordance with the maximum value.", "parent_edge": 105, "level": 4, "size": 22}, "392": {"content": "In scenes where the amount of social activity is small (A, I)\nand those where it is large (E, F), the subjective evaluations were\ngenerally consistent among the evaluators (Figure 6).", "parent_edge": 106, "level": 4, "size": 41}, "393": {"content": "There were\nalso scenes (G, J, C, H) in which evaluations tended to be scattered\nslightly, and scenes (B, D) in which evaluations were varied greatly\namong evaluators.", "parent_edge": 106, "level": 4, "size": 38}, "394": {"content": "The outline of the obtained results is shown\nbelow.", "parent_edge": 106, "level": 4, "size": 10}, "395": {"content": "\u2022 In the scene in which the camera wearer spoke and/or en-\ngaged, the amount of social activity was evaluated much,\nand we confirmed the tendency of the other\u2019s face facing the\ncamera wearer (Figure 6: C, H, D, E, F).", "parent_edge": 107, "level": 4, "size": 53}, "396": {"content": "\u2022 There were scenes that were different from the subjective\nevaluation when the amount of social activity was calculated\nby only counting the number of faces (Figure 6: J, F).", "parent_edge": 108, "level": 4, "size": 35}, "397": {"content": "\u2022 There were scenes in which the social activity quantified\nby both methods was in a different order from that of the\nsubjective evaluation (Figure 6: H, D, E).", "parent_edge": 109, "level": 4, "size": 35}, "398": {"content": "The face was not\ncompletely captured when the dialogue partner of the cam-\nera wearer was too close or the standing position became\noblique.", "parent_edge": 109, "level": 4, "size": 25}, "399": {"content": "The detailed results of the subjective evaluation experiment and", "parent_edge": 110, "level": 4, "size": 9}, "400": {"content": "the scene are described in Sections 4.3.1 and 4.3.2.", "parent_edge": 111, "level": 4, "size": 10}, "401": {"content": "4.4.1\nScene in which the Evaluation was Consistent among the Eval-\nuators.", "parent_edge": 112, "level": 4, "size": 13}, "402": {"content": "In scenes where the amount of social activity is small (A, I)\nor large (E, F), the subjective evaluation generally were consistent\namong the evaluators (Figure 6).", "parent_edge": 112, "level": 4, "size": 37}, "403": {"content": "There were also scenes (G, J, C, H)\nin which evaluations tended to be scattered somewhat.", "parent_edge": 112, "level": 4, "size": 22}, "404": {"content": "When the camera wearer participated in the conversation and\nspoke, the amount of social activity tended to be evaluated as large", "parent_edge": 113, "level": 4, "size": 22}, "405": {"content": "5\fAH2019, March 11\u201312, 2019, Reims, France", "parent_edge": 114, "level": 4, "size": 11}, "406": {"content": "Akane Okuno et al.", "parent_edge": 115, "level": 4, "size": 5}, "407": {"content": "Figure 6: Subjective evaluation results that were quantified by rearranging 10 videos (SE).", "parent_edge": 116, "level": 4, "size": 17}, "408": {"content": "A comparison of SE and the amount\nof social activity by proposed method (PM) and with counting the number of faces (NF).", "parent_edge": 116, "level": 4, "size": 27}, "409": {"content": "by evaluators.", "parent_edge": 117, "level": 4, "size": 3}, "410": {"content": "The description that was often seen in the sorting\ncriteria was \"Whether camera wearer is speaking and/or participating\nin a conversation.\"", "parent_edge": 117, "level": 4, "size": 24}, "411": {"content": "We confirmed the tendency of the conversation\npartner facing the camera wearer when the camera wearer was\nspeaking (Figure 6: C, H, D, E, F).", "parent_edge": 117, "level": 4, "size": 33}, "412": {"content": "Additionally, the amount of social activity of Scenes J and F\nquantified by the proposed method was in the same order as in\nthe subjective evaluation.", "parent_edge": 118, "level": 4, "size": 28}, "413": {"content": "However, the amount of social activity\nquantified by counting the number of faces was different from the\norder of subjective evaluation.", "parent_edge": 118, "level": 4, "size": 23}, "414": {"content": "On the other hand, in Scenes H and E, the social activity quanti-\nfied by both methods was in a different order from the subjective", "parent_edge": 119, "level": 4, "size": 27}, "415": {"content": "evaluation.", "parent_edge": 120, "level": 4, "size": 2}, "416": {"content": "The face was not completely captured when the dia-\nlogue partner of the camera wearer was too close or the standing\nposition became oblique.", "parent_edge": 120, "level": 4, "size": 25}, "417": {"content": "The contents of each scene that was consistently evaluated are", "parent_edge": 121, "level": 4, "size": 10}, "418": {"content": "described below (Scenes A, I, E, F).", "parent_edge": 122, "level": 4, "size": 13}, "419": {"content": "Scene A", "parent_edge": 123, "level": 4, "size": 2}, "420": {"content": "The camera wearer went down the stairs.", "parent_edge": 124, "level": 4, "size": 8}, "421": {"content": "He passed a few\npeople in the hallway.", "parent_edge": 124, "level": 4, "size": 9}, "422": {"content": "He did not talk to anyone.", "parent_edge": 124, "level": 4, "size": 7}, "423": {"content": "Scene I", "parent_edge": 125, "level": 4, "size": 2}, "424": {"content": "The camera wearer listened to the talks behind the conver-\nsation between the presenter and the visitor.", "parent_edge": 126, "level": 4, "size": 18}, "425": {"content": "After looking\naround several times, he moved and read the posters.", "parent_edge": 126, "level": 4, "size": 13}, "426": {"content": "He did\nnot participate in the conversation directly.", "parent_edge": 126, "level": 4, "size": 9}, "427": {"content": "6\fSocial Activity Measurement by Counting Faces Captured in\nFirst-Person View Lifelogging Video", "parent_edge": 127, "level": 4, "size": 13}, "428": {"content": "AH2019, March 11\u201312, 2019, Reims, France", "parent_edge": 128, "level": 4, "size": 10}, "429": {"content": "Scene E", "parent_edge": 129, "level": 4, "size": 2}, "430": {"content": "The camera wearer and Person P1 had a one-on-one conver-\nsation.", "parent_edge": 130, "level": 4, "size": 12}, "431": {"content": "They talked with each other with speech and gestures.", "parent_edge": 130, "level": 4, "size": 10}, "432": {"content": "On the way, the distance between them became closer or\nthe standing position was oblique.", "parent_edge": 130, "level": 4, "size": 17}, "433": {"content": "There was a scene in\nwhich the face of the conversation partner was not within\nthe viewing angle of the camera and could not be captured\nproperly.", "parent_edge": 130, "level": 4, "size": 28}, "434": {"content": "Scene F", "parent_edge": 131, "level": 4, "size": 2}, "435": {"content": "The camera wearer and Person P1 had a one-on-one conver-\nsation.", "parent_edge": 132, "level": 4, "size": 12}, "436": {"content": "Then, another person joined the conversation.", "parent_edge": 132, "level": 4, "size": 8}, "437": {"content": "They\ntalked with each other with speech and gestures.", "parent_edge": 132, "level": 4, "size": 10}, "438": {"content": "Three peo-\nple saw the booklet held by Person P1 and approached.", "parent_edge": 132, "level": 4, "size": 13}, "439": {"content": "There\nwere several scenes in which the face of Person P1 was par-\ntially out of the view angle of the camera.", "parent_edge": 132, "level": 4, "size": 23}, "440": {"content": "The contents of each scene that tended to be scattered somewhat", "parent_edge": 133, "level": 4, "size": 11}, "441": {"content": "in evaluation are described below (Scenes G, J, C, H).", "parent_edge": 134, "level": 4, "size": 16}, "442": {"content": "Scene G", "parent_edge": 135, "level": 4, "size": 2}, "443": {"content": "The camera wearer turned in the direction of the presen-\nter from afar and heard a talk.", "parent_edge": 136, "level": 4, "size": 18}, "444": {"content": "The presenter was facing\nthe direction of the visitor and the camera wearer.", "parent_edge": 136, "level": 4, "size": 14}, "445": {"content": "In the\nfirst half, there were many visitors next door.", "parent_edge": 136, "level": 4, "size": 12}, "446": {"content": "In the second\nhalf, he looked at his surroundings.", "parent_edge": 136, "level": 4, "size": 11}, "447": {"content": "He did not have direct\nconversation or speak.", "parent_edge": 136, "level": 4, "size": 9}, "448": {"content": "Scene J", "parent_edge": 137, "level": 4, "size": 2}, "449": {"content": "The camera wearer turned in the direction of the presenter\nfrom afar and heard a talk.", "parent_edge": 138, "level": 4, "size": 17}, "450": {"content": "The presenter was facing the\ndirection of the visitor and the camera wearer.", "parent_edge": 138, "level": 4, "size": 14}, "451": {"content": "The camera\nwearer was in a position facing the other visitors.", "parent_edge": 138, "level": 4, "size": 12}, "452": {"content": "He did not\nhave direct conversation or speak.", "parent_edge": 138, "level": 4, "size": 9}, "453": {"content": "Scene C", "parent_edge": 139, "level": 4, "size": 2}, "454": {"content": "The camera wearer was talking to the presenter while expe-\nriencing the demonstration.", "parent_edge": 140, "level": 4, "size": 14}, "455": {"content": "They talked to each other.", "parent_edge": 140, "level": 4, "size": 6}, "456": {"content": "He\ndid not have direct conversation, but another presenter and\nvisitor were nearby.", "parent_edge": 140, "level": 4, "size": 15}, "457": {"content": "When he moved, he passed many other\nvisitors.", "parent_edge": 140, "level": 4, "size": 10}, "458": {"content": "Scene H", "parent_edge": 141, "level": 4, "size": 2}, "459": {"content": "In the first half, the camera wearer moved after listening\nto a talk from afar.", "parent_edge": 142, "level": 4, "size": 17}, "460": {"content": "In the second half, he met Person P1\nand talked at a short distance and with an oblique standing\nposition.", "parent_edge": 142, "level": 4, "size": 22}, "461": {"content": "The conversation was short, approximately 20 s.\nThere was a scene in which the face of Person P1 was not\nwithin the viewing angle of the camera and was not captured\nprecisely.", "parent_edge": 142, "level": 4, "size": 34}, "462": {"content": "4.4.2\nScenes in which Subjective Evaluation Did Not Match among\nEvaluators.", "parent_edge": 143, "level": 4, "size": 12}, "463": {"content": "There were scenes (B, D) where the subjective evalua-\ntions were dispersed among the evaluators (Figure 6).", "parent_edge": 143, "level": 4, "size": 23}, "464": {"content": "Thus, there\nwere individual differences in the quantitative impression on several\nsocial activities.", "parent_edge": 143, "level": 4, "size": 15}, "465": {"content": "When we confirmed the actual scene, in Scene B, the two situa-\ntions were mixed in the first half and the second half.", "parent_edge": 144, "level": 4, "size": 26}, "466": {"content": "Additionally,\nin Scene D, the camera wearer talked to the presenter for the whole\ntime while experiencing the demonstration.", "parent_edge": 144, "level": 4, "size": 22}, "467": {"content": "The face of the con-\nversation partner was not within the angle of view of the camera.", "parent_edge": 144, "level": 4, "size": 18}, "468": {"content": "However, the body was facing the front when they talked.", "parent_edge": 144, "level": 4, "size": 12}, "469": {"content": "The contents of each scene for which evaluations did not match", "parent_edge": 145, "level": 4, "size": 11}, "470": {"content": "among evaluators are described below (Scenes B, D).", "parent_edge": 146, "level": 4, "size": 12}, "471": {"content": "Scene B", "parent_edge": 147, "level": 4, "size": 2}, "472": {"content": "In the first half, the camera wearer walked in the crowd and\nmoved toward the presenter.", "parent_edge": 148, "level": 4, "size": 18}, "473": {"content": "In the second half, he experi-\nenced the demonstration in front of a presenter.", "parent_edge": 148, "level": 4, "size": 16}, "474": {"content": "He touched\nthe exhibition along with the presenter and visitor.", "parent_edge": 148, "level": 4, "size": 11}, "475": {"content": "The pre-\nsenter talked to the other visitor next to the camera wearer.", "parent_edge": 148, "level": 4, "size": 14}, "476": {"content": "The camera wearer did not talk with them.", "parent_edge": 148, "level": 4, "size": 9}, "477": {"content": "Scene D", "parent_edge": 149, "level": 4, "size": 2}, "478": {"content": "The camera wearer talked to the presenter for the whole\ntime while experiencing the demonstration.", "parent_edge": 150, "level": 4, "size": 16}, "479": {"content": "A conversation\npartner was standing in front of the seated camera wearer.", "parent_edge": 150, "level": 4, "size": 13}, "480": {"content": "The face of the conversation partner was not within the\nangle of view of the camera and could not be seen.", "parent_edge": 150, "level": 4, "size": 22}, "481": {"content": "However,\nthe conversation partner\u2019s body was facing forward when\nthey talked.", "parent_edge": 150, "level": 4, "size": 15}, "482": {"content": "5 DISCUSSION\n5.1 Scenes in which the Amount of Social", "parent_edge": 151, "level": 4, "size": 10}, "483": {"content": "Activity Is Large", "parent_edge": 152, "level": 4, "size": 3}, "484": {"content": "In the scene in which the camera wearer spoke and/or engaged, the\namount of social activity was evaluated as high, and we confirmed\nthe tendency of the other person facing the camera wearer (Figure 6:\nC, H, D, E, F).", "parent_edge": 153, "level": 4, "size": 50}, "485": {"content": "Especially when the camera wearer himself speaks,\nit is evaluated as a scene with a large amount of social activity.", "parent_edge": 153, "level": 4, "size": 22}, "486": {"content": "The interesting thing here is that our proposed method does not\nmeasure \"utterance amount\" itself.", "parent_edge": 153, "level": 4, "size": 18}, "487": {"content": "Nonetheless, it is possible to\npresent a high score by the proposed method of detecting faces in\nthe conversation scene.", "parent_edge": 153, "level": 4, "size": 22}, "488": {"content": "This is because when the camera wearer\nspeaks, the tendency of the surrounding people to face the camera\nwearer increases, and as a result, faces are detected.", "parent_edge": 153, "level": 4, "size": 31}, "489": {"content": "Thus, it is\nsuggested that social activity can be measured by detecting the face\nof the partner when the camera wearer performs an active behavior,\nwithout measuring utterances or gestures themselves.", "parent_edge": 153, "level": 4, "size": 34}, "490": {"content": "5.2 Active Face-to-Face Engagement\nThe number of people who face each other does not depend much\non the evaluation of the amount of social activity, and the continuity\nand closeness with the same person leads to higher evaluation.", "parent_edge": 154, "level": 4, "size": 40}, "491": {"content": "The\nscoring process works well by not only the number of faces, but\nalso proximity and time continuity (Figure 6: F).", "parent_edge": 154, "level": 4, "size": 26}, "492": {"content": "With a method\nof only counting the number of faces, we scored higher than the\nresult of subjective evaluation in the scene of hearing the talks\nfrom afar (Figure 6: J).", "parent_edge": 154, "level": 4, "size": 36}, "493": {"content": "In order to quantify the amount of social\nactivity, considering the active behavior, it is necessary to weight\nby proximity and time continuity.", "parent_edge": 154, "level": 4, "size": 26}, "494": {"content": "5.3 Consistency of Evaluation by Evaluator\nThe result of subjective evaluation was not significantly affected by\nwhether the evaluator was the camera wearer, dialogue partner, or\nthird party.", "parent_edge": 155, "level": 4, "size": 31}, "495": {"content": "In other words, the evaluation of the amount of social\nactivity is not considered to be affected much by episodes.", "parent_edge": 155, "level": 4, "size": 22}, "496": {"content": "7\fAH2019, March 11\u201312, 2019, Reims, France", "parent_edge": 156, "level": 4, "size": 11}, "497": {"content": "Akane Okuno et al.", "parent_edge": 157, "level": 4, "size": 5}, "498": {"content": "Figure 7: Angle of view for measuring the face-to-face engagement when the standing position is not in front", "parent_edge": 158, "level": 4, "size": 19}, "499": {"content": "5.4 Consistency of Evaluation by Type of Scene\nThe variance of subjective evaluation varied depending on the type\nof scene.", "parent_edge": 159, "level": 4, "size": 21}, "500": {"content": "For example, scenes of long talk with a specific person\nhas a high score (Figure 6: E, F), a scene moving in a hallway has a\nlow score, and everyone makes a common judgment (Figure 6: A).", "parent_edge": 159, "level": 4, "size": 48}, "501": {"content": "Additionally, the evaluation of Scene B, experiencing demonstra-\ntions in front of the presenter after participating in a large group\nas a bystander, and that of Scene D, experiencing demonstration\nsystems while talking with specific people, were quite different.", "parent_edge": 159, "level": 4, "size": 45}, "502": {"content": "Consideration of individual differences in the impression of such a\nscene is the limit of the proposed method.", "parent_edge": 159, "level": 4, "size": 19}, "503": {"content": "5.5 Field of View of First-Person View Camera\nIn the scenes where the dialogue parties are out of camera view, the\nscore of the proposed method differs considerably from the subjec-\ntive evaluation (Figure 6: H, D, E).", "parent_edge": 160, "level": 4, "size": 45}, "504": {"content": "The reason is that the dialogue\npartner\u2019s face has not been captured by dialogue at diagonal or\nadjacent positions at a short distance, or at a position with high\nvertical difference.", "parent_edge": 160, "level": 4, "size": 35}, "505": {"content": "We thought the problem would be improved if the view angle of\nthe camera were expanded.", "parent_edge": 161, "level": 4, "size": 17}, "506": {"content": "For this reason, we measured social ac-\ntivity using a 200 \u25e6 hemispheric camera (Figure 7).", "parent_edge": 161, "level": 4, "size": 20}, "507": {"content": "It was confirmed\nthat the dialogue partner\u2019s face could be detected and measured\nin a one-on-one dialogue with a diagonal standing position.", "parent_edge": 161, "level": 4, "size": 25}, "508": {"content": "Fur-\nthermore, within the university campus, we were able to detect\nside-by-side faces during meals and work, as well as detect faces\nduring dialogue at a position with a vertical difference (Figure 8).", "parent_edge": 161, "level": 4, "size": 39}, "509": {"content": "Thus, we think that the proposed method can also measure the\nface-to-face engagement level when the standing position is not in\nfront.", "parent_edge": 161, "level": 4, "size": 24}, "510": {"content": "However, there is fish-eye distortion in this case, so it is necessary\nto use a robust face detector.", "parent_edge": 162, "level": 4, "size": 21}, "511": {"content": "In addition, because the size of the face\nvaries depending on where the face is captured by the distortion,\nwe are reconsidering the weighting of the face size by removing\nthe distortion or based on the standing position.", "parent_edge": 162, "level": 4, "size": 41}, "512": {"content": "There is a limit,\nas it is not possible to measure the engagement with the person\nbehind.", "parent_edge": 162, "level": 4, "size": 19}, "513": {"content": "6 CONCLUSION\nWe propose a method to measure the daily face-to-face social ac-\ntivity of the camera wearer by detecting faces captured in the first-\nperson view lifelogging video for the purpose of quantifying the\nface-to-face engagement degree with a simple method.", "parent_edge": 163, "level": 4, "size": 43}, "514": {"content": "From the result of the subjective evaluation experiment, it was\nsuggested that social activity can be measured by detecting the face\nof the partner when the camera wearer performs an active behavior\nwithout measuring the speech or gestures themselves.", "parent_edge": 164, "level": 4, "size": 41}, "515": {"content": "Additionally,\nto quantify the amount of social activity considering the active\nbehavior, it is necessary to weight by distance shortness and time\ncontinuity.", "parent_edge": 164, "level": 4, "size": 26}, "516": {"content": "A few diagonal and side-by-side dialogues were found, and it\nwas found that the view angle of 180 \u25e6 or more was necessary.", "parent_edge": 165, "level": 4, "size": 25}, "517": {"content": "As\na result of using a 200 \u25e6 hemispherical camera, it was suggested\nthat the measurement of situations with a high level of face-to-face\nengagement can be improved.", "parent_edge": 165, "level": 4, "size": 30}, "518": {"content": "Measurement of engagement with people facing away and mea-\nsurement considering individual impression differences in a few\nscenes remain the limit of the proposed method.", "parent_edge": 166, "level": 4, "size": 26}, "519": {"content": "In the future, we aim to provide feedback that leads to behavioral\nchanges leading to social health, such as reducing loneliness and\nfatigue in social activities [9].", "parent_edge": 167, "level": 4, "size": 32}, "520": {"content": "As a different viewpoint from complex\nsocial relationships, we think that the amount of social activity\nfacing people is one of the clues to support young people\u2019s social\nwithdrawal [22] and elderly depression [10].", "parent_edge": 167, "level": 4, "size": 42}, "521": {"content": "We believe that our\nproposed method is the first step toward changes in social behavior.", "parent_edge": 167, "level": 4, "size": 16}, "522": {"content": "ACKNOWLEDGMENTS\nThis study was partially supported by Exploratory IT Human Re-\nsources Project (MITOU Program) of Information-technology Pro-\nmotion Agency, Japan (IPA).", "parent_edge": 168, "level": 4, "size": 28}, "523": {"content": "REFERENCES\n[1] Stefano Alletto, Giuseppe Serra, Simone Calderara, Francesco Solera, and Rita\nCucchiara.", "parent_edge": 169, "level": 4, "size": 20}, "524": {"content": "2014.", "parent_edge": 169, "level": 4, "size": 2}, "525": {"content": "From ego to nos-vision: Detecting social relationships in first-\nperson views.", "parent_edge": 169, "level": 4, "size": 13}, "526": {"content": "In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition Workshops.", "parent_edge": 169, "level": 4, "size": 14}, "527": {"content": "580\u2013585.", "parent_edge": 169, "level": 4, "size": 2}, "528": {"content": "[2] Brandon Amos, Bartosz Ludwiczuk, and Mahadev Satyanarayanan.", "parent_edge": 170, "level": 4, "size": 13}, "529": {"content": "2016.", "parent_edge": 170, "level": 4, "size": 2}, "530": {"content": "Open-\nFace: A general-purpose face recognition library with mobile applications.", "parent_edge": 170, "level": 4, "size": 12}, "531": {"content": "CMU\nSchool of Computer Science (2016).", "parent_edge": 170, "level": 4, "size": 9}, "532": {"content": "8\fSocial Activity Measurement by Counting Faces Captured in\nFirst-Person View Lifelogging Video", "parent_edge": 171, "level": 4, "size": 13}, "533": {"content": "AH2019, March 11\u201312, 2019, Reims, France", "parent_edge": 172, "level": 4, "size": 10}, "534": {"content": "Figure 8: Measurement of dialogue at diagonal or side-by-side position", "parent_edge": 173, "level": 4, "size": 11}, "535": {"content": "10th IEEE international symposium on wearable computers, Montreux, Switzerland.", "parent_edge": 174, "level": 4, "size": 12}, "536": {"content": "Citeseer, 11\u201314.", "parent_edge": 174, "level": 4, "size": 4}, "537": {"content": "[18] Gillian O\u2019Loughlin, Sarah Jane Cullen, Adrian McGoldrick, Siobhan O\u2019Connor,\nRichard Blain, Shane O\u2019Malley, and Giles D Warrington.", "parent_edge": 175, "level": 4, "size": 33}, "538": {"content": "2013.", "parent_edge": 175, "level": 4, "size": 2}, "539": {"content": "Using a wearable\ncamera to increase the accuracy of dietary analysis.", "parent_edge": 175, "level": 4, "size": 12}, "540": {"content": "American Journal of Preventive\nMedicine 44, 3 (2013), 297\u2013301.", "parent_edge": 175, "level": 4, "size": 14}, "541": {"content": "[19] Arkadiusz Stopczynski, Vedran Sekara, Piotr Sapiezynski, Andrea Cuttone,\nMette My Madsen, Jakob Eg Larsen, and Sune Lehmann.", "parent_edge": 176, "level": 4, "size": 27}, "542": {"content": "2014.", "parent_edge": 176, "level": 4, "size": 2}, "543": {"content": "Measuring large-\nscale social networks with high resolution.", "parent_edge": 176, "level": 4, "size": 9}, "544": {"content": "PloS one 9, 4 (2014), e95978.", "parent_edge": 176, "level": 4, "size": 11}, "545": {"content": "[20] Yasuyuki Sumi, Masaki Suwa, and Koichi Hanaue.", "parent_edge": 176, "level": 4, "size": 13}, "546": {"content": "2018.", "parent_edge": 176, "level": 4, "size": 2}, "547": {"content": "Effects of Viewing\nMultiple Viewpoint Videos on Metacognition of Collaborative Experiences.", "parent_edge": 176, "level": 4, "size": 12}, "548": {"content": "In\nProceedings of the 2018 CHI Conference on Human Factors in Computing Systems\n(CHI \u201918).", "parent_edge": 176, "level": 4, "size": 19}, "549": {"content": "ACM, New York, NY, USA, Article 648, 13 pages.", "parent_edge": 176, "level": 4, "size": 15}, "550": {"content": "https://doi.org/10.", "parent_edge": 176, "level": 4, "size": 4}, "551": {"content": "1145/3173574.3174222", "parent_edge": 176, "level": 4, "size": 1}, "552": {"content": "[21] Girmaw Abebe Tadesse and Andrea Cavallaro.", "parent_edge": 177, "level": 4, "size": 10}, "553": {"content": "2018.", "parent_edge": 177, "level": 4, "size": 2}, "554": {"content": "Visual Features for Ego-\ncentric Activity Recognition: A Survey.", "parent_edge": 177, "level": 4, "size": 11}, "555": {"content": "In Proceedings of the 4th ACM Workshop\non Wearable Systems and Applications (WearSys \u201918).", "parent_edge": 177, "level": 4, "size": 18}, "556": {"content": "ACM, New York, NY, USA,\n48\u201353.", "parent_edge": 177, "level": 4, "size": 11}, "557": {"content": "https://doi.org/10.1145/3211960.3211978", "parent_edge": 177, "level": 4, "size": 3}, "558": {"content": "[22] Alan Robert Teo and Albert C Gaw.", "parent_edge": 178, "level": 4, "size": 11}, "559": {"content": "2010.", "parent_edge": 178, "level": 4, "size": 2}, "560": {"content": "Hikikomori, A Japanese Culture-Bound\nSyndrome of Social Withdrawal?", "parent_edge": 178, "level": 4, "size": 10}, "561": {"content": "A Proposal for DSM-V.", "parent_edge": 178, "level": 4, "size": 5}, "562": {"content": "The Journal of Nervous\nand Mental Disease 198, 6 (2010), 444.", "parent_edge": 178, "level": 4, "size": 16}, "563": {"content": "[23] Ryo Yonetani, Kris M Kitani, and Yoichi Sato.", "parent_edge": 179, "level": 4, "size": 14}, "564": {"content": "2015.", "parent_edge": 179, "level": 4, "size": 2}, "565": {"content": "Ego-surfing first person\nvideos.", "parent_edge": 179, "level": 4, "size": 5}, "566": {"content": "In Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference\non.", "parent_edge": 179, "level": 4, "size": 15}, "567": {"content": "IEEE, 5445\u20135454.", "parent_edge": 179, "level": 4, "size": 4}, "568": {"content": "[3] Tanzeem Choudhury and Alex Pentland.", "parent_edge": 180, "level": 4, "size": 9}, "569": {"content": "2003.", "parent_edge": 180, "level": 4, "size": 2}, "570": {"content": "Sensing and Modeling Human\nNetworks Using the Sociometer.", "parent_edge": 180, "level": 4, "size": 9}, "571": {"content": "In Proceedings of the 7th IEEE International Sym-\nposium on Wearable Computers (ISWC \u201903).", "parent_edge": 180, "level": 4, "size": 18}, "572": {"content": "IEEE Computer Society, Washington,\nDC, USA, 216\u2013.", "parent_edge": 180, "level": 4, "size": 12}, "573": {"content": "http://dl.acm.org/citation.cfm?id=946249.946901", "parent_edge": 180, "level": 4, "size": 5}, "574": {"content": "[4] Martin Danelljan, Gustav H\u00e4ger, Fahad Khan, and Michael Felsberg.", "parent_edge": 181, "level": 4, "size": 16}, "575": {"content": "2014.", "parent_edge": 181, "level": 4, "size": 2}, "576": {"content": "Ac-\ncurate scale estimation for robust visual tracking.", "parent_edge": 181, "level": 4, "size": 9}, "577": {"content": "In British Machine Vision\nConference, Nottingham, September 1-5, 2014.", "parent_edge": 181, "level": 4, "size": 13}, "578": {"content": "BMVA Press.", "parent_edge": 181, "level": 4, "size": 3}, "579": {"content": "[5] Nathan Eagle and Alex Sandy Pentland.", "parent_edge": 182, "level": 4, "size": 10}, "580": {"content": "2009.", "parent_edge": 182, "level": 4, "size": 2}, "581": {"content": "Eigenbehaviors: Identifying struc-\nture in routine.", "parent_edge": 182, "level": 4, "size": 8}, "582": {"content": "Behavioral Ecology and Sociobiology 63, 7 (2009), 1057\u20131066.", "parent_edge": 182, "level": 4, "size": 13}, "583": {"content": "[6] Alircza Fathi, Jessica K Hodgins, and James M Rehg.", "parent_edge": 182, "level": 4, "size": 15}, "584": {"content": "2012.", "parent_edge": 182, "level": 4, "size": 2}, "585": {"content": "Social interactions: A\nfirst-person perspective.", "parent_edge": 182, "level": 4, "size": 7}, "586": {"content": "In Computer Vision and Pattern Recognition (CVPR), 2012\nIEEE Conference on.", "parent_edge": 182, "level": 4, "size": 15}, "587": {"content": "IEEE, 1226\u20131233.", "parent_edge": 182, "level": 4, "size": 4}, "588": {"content": "[7] Fangfang Guo, Yu Li, Mohan S. Kankanhalli, and Michael S. Brown.", "parent_edge": 183, "level": 4, "size": 18}, "589": {"content": "2013.", "parent_edge": 183, "level": 4, "size": 2}, "590": {"content": "An\nEvaluation of Wearable Activity Monitoring Devices.", "parent_edge": 183, "level": 4, "size": 8}, "591": {"content": "In Proceedings of the 1st\nACM International Workshop on Personal Data Meets Distributed Multimedia (PDM\n\u201913).", "parent_edge": 183, "level": 4, "size": 20}, "592": {"content": "ACM, New York, NY, USA, 31\u201334.", "parent_edge": 183, "level": 4, "size": 11}, "593": {"content": "https://doi.org/10.1145/2509352.2512882\n[8] Steve Hodges, Lyndsay Williams, Emma Berry, Shahram Izadi, James Srini-\nvasan, Alex Butler, Gavin Smyth, Narinder Kapur, and Ken Woodberry.", "parent_edge": 183, "level": 4, "size": 35}, "594": {"content": "2006.", "parent_edge": 183, "level": 4, "size": 2}, "595": {"content": "SenseCam: A Retrospective Memory Aid.", "parent_edge": 183, "level": 4, "size": 7}, "596": {"content": "https://www.microsoft.com/en-\nus/research/publication/sensecam-a-retrospective-memory-aid/, In Proceedings\nof the 8th International Conference of Ubiquitous Computing (UbiComp 2006).", "parent_edge": 183, "level": 4, "size": 20}, "597": {"content": "177\u2013193.", "parent_edge": 183, "level": 4, "size": 2}, "598": {"content": "[9] James S House, Karl R Landis, and Debra Umberson.", "parent_edge": 184, "level": 4, "size": 15}, "599": {"content": "1988.", "parent_edge": 184, "level": 4, "size": 2}, "600": {"content": "Social relationships", "parent_edge": 184, "level": 4, "size": 2}, "601": {"content": "and health.", "parent_edge": 185, "level": 4, "size": 3}, "602": {"content": "Science 241, 4865 (1988), 540\u2013545.", "parent_edge": 185, "level": 4, "size": 10}, "603": {"content": "[10] Tatsuhiko Kaji, Kazuo Mishima, Shingo Kitamura, Minori Enomoto, Yukihiro\nNagase, Lan Li, Yoshitaka Kaneita, Takashi Ohida, Toru Nishikawa, and Makoto\nUchiyama.", "parent_edge": 186, "level": 4, "size": 34}, "604": {"content": "2010.", "parent_edge": 186, "level": 4, "size": 2}, "605": {"content": "Relationship between late-life depression and life stressors:\nLarge-scale cross-sectional study of a representative sample of the Japanese\ngeneral population.", "parent_edge": 186, "level": 4, "size": 21}, "606": {"content": "Psychiatry and clinical neurosciences 64, 4 (2010), 426\u2013434.", "parent_edge": 186, "level": 4, "size": 13}, "607": {"content": "[11] Vaiva Kalnikaite, Abigail Sellen, Steve Whittaker, and David Kirk.", "parent_edge": 186, "level": 4, "size": 16}, "608": {"content": "2010.", "parent_edge": 186, "level": 4, "size": 2}, "609": {"content": "Now\nLet Me See Where I Was: Understanding How Lifelogs Mediate Memory.", "parent_edge": 186, "level": 4, "size": 14}, "610": {"content": "In\nProceedings of the SIGCHI Conference on Human Factors in Computing Systems\n(CHI \u201910).", "parent_edge": 186, "level": 4, "size": 18}, "611": {"content": "ACM, New York, NY, USA, 2045\u20132054.", "parent_edge": 186, "level": 4, "size": 11}, "612": {"content": "https://doi.org/10.1145/1753326.", "parent_edge": 186, "level": 4, "size": 4}, "613": {"content": "1753638", "parent_edge": 186, "level": 4, "size": 1}, "614": {"content": "[12] Shunichi Kasahara, Mitsuhito Ando, Kiyoshi Suganuma, and Jun Rekimoto.", "parent_edge": 187, "level": 4, "size": 16}, "615": {"content": "2016.", "parent_edge": 187, "level": 4, "size": 2}, "616": {"content": "Parallel Eyes: Exploring Human Capability and Behaviors with Paralleled First\nPerson View Sharing.", "parent_edge": 187, "level": 4, "size": 15}, "617": {"content": "In Proceedings of the 2016 CHI Conference on Human Factors\nin Computing Systems (CHI \u201916).", "parent_edge": 187, "level": 4, "size": 19}, "618": {"content": "ACM, New York, NY, USA, 1561\u20131572.", "parent_edge": 187, "level": 4, "size": 11}, "619": {"content": "https:\n//doi.org/10.1145/2858036.2858495", "parent_edge": 187, "level": 4, "size": 3}, "620": {"content": "[13] Katsutoshi Masai, Yuta Sugiura, Masa Ogata, Kai Kunze, Masahiko Inami, and\nMaki Sugimoto.", "parent_edge": 188, "level": 4, "size": 22}, "621": {"content": "2016.", "parent_edge": 188, "level": 4, "size": 2}, "622": {"content": "Facial Expression Recognition in Daily Life by Embedded\nPhoto Reflective Sensors on Smart Eyewear.", "parent_edge": 188, "level": 4, "size": 15}, "623": {"content": "In Proceedings of the 21st International\nConference on Intelligent User Interfaces (IUI \u201916).", "parent_edge": 188, "level": 4, "size": 17}, "624": {"content": "ACM, New York, NY, USA, 317\u2013\n326. https://doi.org/10.1145/2856767.2856770", "parent_edge": 188, "level": 4, "size": 14}, "625": {"content": "[14] Toshiya Nakakura, Yasuyuki Sumi, and Toyoaki Nishida.", "parent_edge": 189, "level": 4, "size": 13}, "626": {"content": "2011.", "parent_edge": 189, "level": 4, "size": 2}, "627": {"content": "Neary: Conversa-\ntional field detection based on situated sound similarity.", "parent_edge": 189, "level": 4, "size": 12}, "628": {"content": "IEICE Transactions on\nInformation and Systems 94, 6 (2011), 1164\u20131172.", "parent_edge": 189, "level": 4, "size": 15}, "629": {"content": "[15] Daniel Olgu\u00edn, Benjamin N Waber, Taemie Kim, Akshay Mohan, Koji Ara, and\nAlex Pentland.", "parent_edge": 190, "level": 4, "size": 23}, "630": {"content": "2009.", "parent_edge": 190, "level": 4, "size": 2}, "631": {"content": "Sensible organizations: Technology and methodology for\nautomatically measuring organizational behavior.", "parent_edge": 190, "level": 4, "size": 12}, "632": {"content": "IEEE Transactions on Systems,\nMan, and Cybernetics, Part B (Cybernetics) 39, 1 (2009), 43\u201355.", "parent_edge": 190, "level": 4, "size": 24}, "633": {"content": "[16] D Olguin Olguin, Joseph A Paradiso, and Alex Pentland.", "parent_edge": 191, "level": 4, "size": 15}, "634": {"content": "2006.", "parent_edge": 191, "level": 4, "size": 2}, "635": {"content": "Wearable commu-\nnicator badge: Designing a new platform for revealing organizational dynamics.", "parent_edge": 191, "level": 4, "size": 14}, "636": {"content": "In Proceedings of the 10th international symposium on wearable computers (student\ncolloquium).", "parent_edge": 191, "level": 4, "size": 15}, "637": {"content": "4\u20136.", "parent_edge": 191, "level": 4, "size": 2}, "638": {"content": "[17] Daniel Olgu\u0131n Olgu\u0131n and Alex Sandy Pentland.", "parent_edge": 192, "level": 4, "size": 11}, "639": {"content": "2006.", "parent_edge": 192, "level": 4, "size": 2}, "640": {"content": "Human activity recognition:\nAccuracy across common locations for wearable sensors.", "parent_edge": 192, "level": 4, "size": 12}, "641": {"content": "In Proceedings of 2006", "parent_edge": 192, "level": 4, "size": 4}, "642": {"content": "9", "parent_edge": 193, "level": 4, "size": 1}, "0": {"level": 1, "name": "front", "parent_edge": -1, "child_edge": [22, 23, 24, 25, 26, 27, 28, 29, 30, 31], "size": 664}}