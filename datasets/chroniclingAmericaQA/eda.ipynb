{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64, gzip, json, os, re, sys\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "# can be repeated on training dataset\n",
    "dataset = \"dev\"\n",
    "DATA_DIR = os.path.join(os.path.expanduser(\"~\"), \"data\", \"chroniclingamericaQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ca_data(dataset):\n",
    "\n",
    "    parsed_objects = []\n",
    "    file_path = f\"{DATA_DIR}/{dataset}.json\"\n",
    "    # Determine file opening method based on extension\n",
    "    open_method = gzip.open if file_path.endswith('.gz') else open\n",
    "    mode = 'rt' if file_path.endswith('.gz') else 'r'\n",
    "    with open_method(file_path, mode, encoding='utf-8') as file:\n",
    "        # Read the entire file content\n",
    "        content = file.read()\n",
    "    # Split the content into lines or use a streaming approach\n",
    "    lines = content.splitlines()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            # Try to parse each line as a separate JSON object\n",
    "            parsed_object = json.loads(line.strip())\n",
    "            parsed_objects.append(parsed_object)\n",
    "        except JSONDecodeError:\n",
    "            # If line parsing fails, try parsing entire content as a single JSON\n",
    "            if not parsed_objects:\n",
    "                try:\n",
    "                    parsed_objects = json.loads(content)\n",
    "                    break\n",
    "                except JSONDecodeError:\n",
    "                    continue\n",
    "    # If no objects parsed, return an empty list\n",
    "    return parsed_objects if parsed_objects else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_qa_dataset(dataset_name: str, dataset_type: str):\n",
    "    \"\"\"\n",
    "    Load QA dataset based on name\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: One of 'hotpotqa', 'chroniclingamericaqa', or 'natural-questions'\n",
    "        data_dir: Base directory containing the datasets\n",
    "    \"\"\"\n",
    "    dataset_loaders = {\n",
    "        'hotpot': lambda: load_hp_data(dataset_type),\n",
    "        'chroniclingamerica': lambda: load_ca_data(dataset_type),\n",
    "        'naturalquestions': lambda: load_nq_data(dataset_type)\n",
    "    }\n",
    "    \n",
    "    dataset_name = dataset_name.lower()  # normalize input\n",
    "    if dataset_name not in dataset_loaders:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}. Available datasets: {list(dataset_loaders.keys())}\")\n",
    "    \n",
    "    return dataset_loaders[dataset_name]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dev_data = load_qa_dataset(\"chroniclingamerica\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_keys(example, dataset_name):\n",
    "    \"\"\"\n",
    "    Standardize the keys of a QA dataset example.\n",
    "    \n",
    "    Args:\n",
    "        example (dict): A QA dataset example\n",
    "        dataset_type (str): Type of the dataset (e.g., 'squad', 'nq', 'triviaqa')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Standardized example with keys 'title', 'context', 'question', 'answers'\n",
    "    \"\"\"\n",
    "    if dataset_name == 'chroniclingamerica':\n",
    "        return {\n",
    "            'x_id': example['query_id'],\n",
    "            'question': example['question'],\n",
    "            'answer': example['answer'],\n",
    "            'context': example['context']\n",
    "        }\n",
    "   \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset type: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_name: str, dataset_type: str):\n",
    "    \"\"\"\n",
    "    Load and standardize a dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        data = load_qa_dataset(dataset_name, dataset_type)\n",
    "        \n",
    "        if data is None:\n",
    "            print(\"Warning: load_qa_dataset returned None\")\n",
    "            return []\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            if len(data) > 0:\n",
    "                print(f\"First item keys: {data[0].keys()}\")\n",
    "            standardized = [standardize_keys(example, dataset_name) for example in data]\n",
    "            print(f\"Standardized {len(standardized)} examples\")\n",
    "            return standardized\n",
    "        else:\n",
    "            return [standardize_keys(example, dataset_name) for example in data]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test it\n",
    "result = process_dataset('chroniclingamerica', 'dev')\n",
    "print(f\"Final result length: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = process_dataset(\"chroniclingamerica\", dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
