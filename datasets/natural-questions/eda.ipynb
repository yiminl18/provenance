{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64, gzip, json, os, re, sys\n",
    "import text_utils as tu\n",
    "import eval_utils as eu\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from json.decoder import JSONDecodeError\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import jinja2\n",
    "import numpy as np\n",
    "\n",
    "# can be repeated on training dataset\n",
    "dataset = \"dev\"\n",
    "DATA_DIR = os.path.join(os.path.expanduser(\"~\"), \"data\", \"v1.0\", f\"{dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    \"\"\"\n",
    "    Robustly read and parse a JSON or gzipped JSON file with multiple JSON objects.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON or gzipped JSON file\n",
    "    \n",
    "    Returns:\n",
    "        list: List of parsed JSON objects\n",
    "    \"\"\"\n",
    "    parsed_objects = []\n",
    "    \n",
    "    # Determine file opening method based on extension\n",
    "    open_method = gzip.open if file_path.endswith('.gz') else open\n",
    "    mode = 'rt' if file_path.endswith('.gz') else 'r'\n",
    "    \n",
    "    with open_method(file_path, mode, encoding='utf-8') as file:\n",
    "        # Read the entire file content\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split the content into lines or use a streaming approach\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        try:\n",
    "            # Try to parse each line as a separate JSON object\n",
    "            parsed_object = json.loads(line.strip())\n",
    "            parsed_objects.append(parsed_object)\n",
    "        except JSONDecodeError:\n",
    "            # If line parsing fails, try parsing entire content as a single JSON\n",
    "            if not parsed_objects:\n",
    "                try:\n",
    "                    parsed_objects = json.loads(content)\n",
    "                    break\n",
    "                except JSONDecodeError:\n",
    "                    continue\n",
    "    \n",
    "    # If no objects parsed, return an empty list\n",
    "    return parsed_objects if parsed_objects else []\n",
    "\n",
    "# dict_keys(['annotations', 'document_html', 'document_title', 'document_tokens', 'document_url', 'example_id', 'long_answer_candidates', 'question_text', 'question_tokens'])\n",
    "#dev_nq = read_json('nq-dev-sample.jsonl.gz')\n",
    "#dev_nq_simplified = read_json('simplified-nq-dev-sample.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_simplified_nq(data_dir, dataset=\"dev\", start_file=0, end_file=None):\n",
    "    \"\"\"\n",
    "    Read simplified Natural Questions data from gzipped JSON files.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the simplified files\n",
    "        dataset: 'train' or 'dev'\n",
    "        start_file: Starting file number (e.g., 0 for nq-{dataset}-00)\n",
    "        end_file: Ending file number (None to read all files)\n",
    "    \n",
    "    Yields:\n",
    "        dict: Each simplified NQ example\n",
    "    \"\"\"\n",
    "    pattern = f\"simplified-nq-{dataset}-??.jsonl.gz\"\n",
    "    files = sorted(Path(data_dir).glob(pattern))\n",
    "    \n",
    "    # Extract file number using string operations\n",
    "    def get_file_num(filepath):\n",
    "        # Get the two digits before .jsonl.gz\n",
    "        return int(filepath.name.split('.')[0][-2:])\n",
    "    \n",
    "    # Filter files based on start/end numbers if specified\n",
    "    if end_file is not None:\n",
    "        files = [f for f in files if get_file_num(f) <= end_file]\n",
    "    files = [f for f in files if get_file_num(f) >= start_file]\n",
    "    \n",
    "    for file in files:\n",
    "        print(f\"Reading {file.name}\")\n",
    "        with gzip.open(file, 'rt', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                yield json.loads(line.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_text(simplified_example):\n",
    "    \"\"\"\n",
    "    Gets all answer texts from a simplified NQ example without annotator agreement requirements.\n",
    "    \n",
    "    Args:\n",
    "        simplified_example: Output from simplify_nq_example()\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing answer texts and metadata\n",
    "    \"\"\"\n",
    "    tokens = tu.get_nq_tokens(simplified_example)\n",
    "    \n",
    "    result = {\n",
    "        'example_id': simplified_example['example_id'],\n",
    "        'question': simplified_example['question_text'],\n",
    "        'document_title': simplified_example['document_title'],\n",
    "        'long_answers': [],\n",
    "        'short_answers': [],\n",
    "        'yes_no_answers': []\n",
    "    }\n",
    "    \n",
    "    for annotation in simplified_example['annotations']:\n",
    "        # Get long answer\n",
    "        long_answer = annotation['long_answer']\n",
    "        if long_answer['start_token'] != -1:\n",
    "            text = ' '.join(tokens[long_answer['start_token']:long_answer['end_token']])\n",
    "            if text not in result['long_answers']:\n",
    "                result['long_answers'].append(text)\n",
    "        \n",
    "        # Get short answers\n",
    "        for short_answer in annotation['short_answers']:\n",
    "            text = ' '.join(tokens[short_answer['start_token']:short_answer['end_token']])\n",
    "            if text not in result['short_answers']:\n",
    "                result['short_answers'].append(text)\n",
    "        \n",
    "        # Get yes/no answer\n",
    "        if 'yes_no_answer' in annotation and annotation['yes_no_answer'] != 'NONE':\n",
    "            result['yes_no_answers'].append(annotation['yes_no_answer'])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_nqex = read_simplified_nq(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_spans_to_dict(obj):\n",
    "    \"\"\"Convert Span objects and other non-serializable objects to dictionaries.\"\"\"\n",
    "    if hasattr(obj, '__dict__'):\n",
    "        # Convert custom objects to their dictionary representation\n",
    "        return {k: convert_spans_to_dict(v) for k, v in obj.__dict__.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_spans_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_spans_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_combine(file_path):\n",
    "    \"\"\"\n",
    "    Process a single simplified NQ file and combine with annotations and answer texts\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    # Get annotations for this file\n",
    "    annotations = eu.read_annotation(str(file_path))\n",
    "    \n",
    "    # Read simplified examples\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            example = json.loads(line)\n",
    "            example_id = example['example_id']\n",
    "            \n",
    "            # Add annotations if they exist eu.gold_has_short_answer(dev_nq_eval[e_id_test])\n",
    "            if example_id in annotations:\n",
    "                example['gold_has_long_answer'] = eu.gold_has_long_answer(annotations[example_id])\n",
    "                example['gold_has_short_answer'] = eu.gold_has_short_answer(annotations[example_id])\n",
    "                \n",
    "            \n",
    "            # Get answer text\n",
    "            answer_text = get_answer_text(example)\n",
    "            example['processed_answers'] = {\n",
    "                'long_answers': answer_text['long_answers'],\n",
    "                'short_answers': answer_text['short_answers'],\n",
    "                'yes_no_answers': answer_text['yes_no_answers']\n",
    "            }\n",
    "            \n",
    "            combined_data.append(example)\n",
    "    \n",
    "    # Save combined data to new file\n",
    "    output_path = str(file_path).replace('simplified-', 'combined-')\n",
    "    with gzip.open(output_path, 'wt', encoding='utf-8') as f:\n",
    "        for item in combined_data:\n",
    "            try:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "            except TypeError as e:\n",
    "                print(f\"Error serializing item: {e}\")\n",
    "                # Optionally print problematic keys\n",
    "                for k, v in item.items():\n",
    "                    try:\n",
    "                        json.dumps(v)\n",
    "                    except TypeError:\n",
    "                        print(f\"Problem with key: {k}, type: {type(v)}\")\n",
    "    \n",
    "    return output_path, len(combined_data)\n",
    "\n",
    "# Process all files with progress tracking\n",
    "pattern = f\"simplified-nq-{dataset}-??.jsonl.gz\"\n",
    "files = sorted(Path(DATA_DIR).glob(pattern))\n",
    "total_processed = 0\n",
    "\n",
    "for file in files:\n",
    "    print(f\"\\nProcessing {file}\")\n",
    "    output_file, num_processed = process_and_combine(file)\n",
    "    total_processed += num_processed\n",
    "    print(f\"Processed {num_processed} examples\")\n",
    "    print(f\"Saved combined data to {output_file}\")\n",
    "\n",
    "print(f\"\\nTotal examples processed: {total_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "original_file = Path(DATA_DIR) / f\"simplified-nq-{dataset}-00.jsonl.gz\"\n",
    "simplified_file = Path(DATA_DIR) / f\"simplified-nq-{dataset}-00.jsonl.gz\"\n",
    "combined_file = Path(DATA_DIR) / f\"combined-nq-{dataset}-00.jsonl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_og = read_json(str(original_file))\n",
    "nq_simplified = read_json(str(simplified_file))\n",
    "nq_combined = read_json(str(combined_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_og[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_simplified[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_combined[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_files(original_file, combined_file, num_examples=1):\n",
    "    \"\"\"\n",
    "    Compare original simplified file with combined file\n",
    "    \n",
    "    Args:\n",
    "        original_file: Path to simplified NQ file\n",
    "        combined_file: Path to combined file with annotations and answers\n",
    "        num_examples: Number of examples to display\n",
    "    \"\"\"\n",
    "    print(f\"Reading from:\\n  Original: {original_file}\\n  Combined: {combined_file}\\n\")\n",
    "    \n",
    "    # Read examples from both files\n",
    "    with gzip.open(original_file, 'rt', encoding='utf-8') as f:\n",
    "        original = [json.loads(line) for line in f][:num_examples]\n",
    "        \n",
    "    with gzip.open(combined_file, 'rt', encoding='utf-8') as f:\n",
    "        combined = [json.loads(line) for line in f][:num_examples]\n",
    "    \n",
    "    # Compare and display\n",
    "    for i, (orig, comb) in enumerate(zip(original, combined)):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(\"\\nOriginal keys:\", list(orig.keys()))\n",
    "        print(\"Combined keys:\", list(comb.keys()))\n",
    "        \n",
    "        # Show what was added\n",
    "        new_keys = set(comb.keys()) - set(orig.keys())\n",
    "        print(\"\\nNewly added fields:\", new_keys)\n",
    "        \n",
    "        # Display some key fields\n",
    "        print(f\"\\nQuestion: {comb['question_text']}\")\n",
    "        print(\"\\nProcessed Answers:\")\n",
    "        print(json.dumps(comb['processed_answers'], indent=2))\n",
    "        \n",
    "        if 'annotation_eval' in comb:\n",
    "            print(\"\\nAnnotation Eval:\")\n",
    "            print(json.dumps(comb['annotation_eval'], indent=2))\n",
    "\n",
    "# Example usage:\n",
    "original_file = Path(DATA_DIR) / f\"simplified-nq-{dataset}-00.jsonl.gz\"\n",
    "combined_file = Path(DATA_DIR) / f\"combined-nq-{dataset}-00.jsonl.gz\"\n",
    "inspect_files(original_file, combined_file, num_examples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    \"\"\"\n",
    "    Robustly read and parse a JSON or gzipped JSON file with multiple JSON objects.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON or gzipped JSON file\n",
    "    \n",
    "    Returns:\n",
    "        list: List of parsed JSON objects\n",
    "    \"\"\"\n",
    "    parsed_objects = []\n",
    "    \n",
    "    # Determine file opening method based on extension\n",
    "    open_method = gzip.open if file_path.endswith('.gz') else open\n",
    "    mode = 'rt' if file_path.endswith('.gz') else 'r'\n",
    "    \n",
    "    with open_method(file_path, mode, encoding='utf-8') as file:\n",
    "        # Read the entire file content\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split the content into lines or use a streaming approach\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        try:\n",
    "            # Try to parse each line as a separate JSON object\n",
    "            parsed_object = json.loads(line.strip())\n",
    "            parsed_objects.append(parsed_object)\n",
    "        except JSONDecodeError:\n",
    "            # If line parsing fails, try parsing entire content as a single JSON\n",
    "            if not parsed_objects:\n",
    "                try:\n",
    "                    parsed_objects = json.loads(content)\n",
    "                    break\n",
    "                except JSONDecodeError:\n",
    "                    continue\n",
    "    \n",
    "    # If no objects parsed, return an empty list\n",
    "    return parsed_objects if parsed_objects else []\n",
    "\n",
    "# dict_keys(['annotations', 'document_html', 'document_title', 'document_tokens', 'document_url', 'example_id', 'long_answer_candidates', 'question_text', 'question_tokens'])\n",
    "dev_nq = read_json('nq-dev-sample.jsonl.gz')\n",
    "dev_nq_simplified = read_json('simplified-nq-dev-sample.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_answer_distribution(data_dir, dataset):\n",
    "    \"\"\"\n",
    "    Count examples with different combinations of answer types\n",
    "    \"\"\"\n",
    "    pattern = f\"combined-nq-{dataset}-??.jsonl.gz\"\n",
    "    files = sorted(Path(data_dir).glob(pattern))\n",
    "    \n",
    "    counts = {\n",
    "        'total': 0,\n",
    "        'has_long': 0,\n",
    "        'has_short': 0,\n",
    "        'has_both': 0,\n",
    "        'has_neither': 0,\n",
    "        'only_long': 0,\n",
    "        'only_short': 0\n",
    "    }\n",
    "    \n",
    "    for file in files:\n",
    "        print(f\"Processing {file}\")\n",
    "        with gzip.open(file, 'rt', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                example = json.loads(line)\n",
    "                answers = example['processed_answers']\n",
    "                \n",
    "                has_long = len(answers['long_answers']) > 0\n",
    "                has_short = len(answers['short_answers']) > 0\n",
    "                \n",
    "                counts['total'] += 1\n",
    "                if has_long: counts['has_long'] += 1\n",
    "                if has_short: counts['has_short'] += 1\n",
    "                if has_long and has_short: counts['has_both'] += 1\n",
    "                if not has_long and not has_short: counts['has_neither'] += 1\n",
    "                if has_long and not has_short: counts['only_long'] += 1\n",
    "                if has_short and not has_long: counts['only_short'] += 1\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nAnswer Distribution:\")\n",
    "    print(f\"Total examples: {counts['total']}\")\n",
    "    print(f\"Examples with long answers: {counts['has_long']} ({counts['has_long']/counts['total']*100:.1f}%)\")\n",
    "    print(f\"Examples with short answers: {counts['has_short']} ({counts['has_short']/counts['total']*100:.1f}%)\")\n",
    "    print(f\"Examples with both types: {counts['has_both']} ({counts['has_both']/counts['total']*100:.1f}%)\")\n",
    "    print(f\"Examples with neither type: {counts['has_neither']} ({counts['has_neither']/counts['total']*100:.1f}%)\")\n",
    "    print(f\"Examples with only long answers: {counts['only_long']} ({counts['only_long']/counts['total']*100:.1f}%)\")\n",
    "    print(f\"Examples with only short answers: {counts['only_short']} ({counts['only_short']/counts['total']*100:.1f}%)\")\n",
    "    \n",
    "    return counts\n",
    "\n",
    "# Use it\n",
    "stats = analyze_answer_distribution(DATA_DIR, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_id_test = simplified_dev_nq[0]['example_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores of zero by default, still should use gold_has_X_answer functions for each example_id\n",
    "dev_nq_eval = eu.read_annotation('nq-dev-sample.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each example_id has a 5 element list for each annotation with bytes/tokens to each short and/or long answer\n",
    "dev_nq_eval[e_id_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu.gold_has_short_answer(dev_nq_eval[e_id_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu.gold_has_long_answer(dev_nq_eval[e_id_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each example_id is a key for an element of dev_nq\n",
    "# a dict with the keys: (['question_text', 'example_id', 'document_url', 'document_text', 'long_answer_candidates', 'annotations'])\n",
    "# long answer candidates is a list of dicts with keys: ['start_token', 'top_level', 'end_token']\n",
    "# annotations is a list of dicts with keys: ['annotation_id', 'yes_no_answer', 'long_answer', 'short_answers']\n",
    "# iterate over annotations's long_answer and short_answers to get the start and end tokens\n",
    "simplified_example = tu.simplify_nq_example(dev_nq[0])  # Simplify first example\n",
    "simplified_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_answer_text(simplified_example):\n",
    "    \"\"\"\n",
    "    Gets all answer texts from a simplified NQ example without annotator agreement requirements.\n",
    "    \n",
    "    Args:\n",
    "        simplified_example: Output from simplify_nq_example()\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing answer texts and metadata\n",
    "    \"\"\"\n",
    "    tokens = tu.get_nq_tokens(simplified_example)\n",
    "    \n",
    "    result = {\n",
    "        'example_id': simplified_example['example_id'],\n",
    "        'question': simplified_example['question_text'],\n",
    "        'document_title': simplified_example['document_title'],\n",
    "        'long_answers': [],\n",
    "        'short_answers': [],\n",
    "        'yes_no_answers': []\n",
    "    }\n",
    "    \n",
    "    for annotation in simplified_example['annotations']:\n",
    "        # Get long answer\n",
    "        long_answer = annotation['long_answer']\n",
    "        if long_answer['start_token'] != -1:\n",
    "            text = ' '.join(tokens[long_answer['start_token']:long_answer['end_token']])\n",
    "            if text not in result['long_answers']:\n",
    "                result['long_answers'].append(text)\n",
    "        \n",
    "        # Get short answers\n",
    "        for short_answer in annotation['short_answers']:\n",
    "            text = ' '.join(tokens[short_answer['start_token']:short_answer['end_token']])\n",
    "            if text not in result['short_answers']:\n",
    "                result['short_answers'].append(text)\n",
    "        \n",
    "        # Get yes/no answer\n",
    "        if 'yes_no_answer' in annotation and annotation['yes_no_answer'] != 'NONE':\n",
    "            result['yes_no_answers'].append(annotation['yes_no_answer'])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_text_test = get_simple_answer_text(example)\n",
    "answer_text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_texts = get_simple_answer_text(simplified_example)\n",
    "answer_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_label_examples = []\n",
    "\n",
    "for a in simplified_example['annotations']:\n",
    "    e_id = simplified_example['example_id']\n",
    "    simple_span = eu.Span(-1, -1, a['long_anser']['start_token'], a['long_answer']['end_token'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"Question: {answer_texts['question']}\")\n",
    "if answer_texts['long_answer_text']:\n",
    "    print(f\"\\nLong answer: {answer_texts['long_answer_text']}\")\n",
    "if answer_texts['short_answers_text']:\n",
    "    print(f\"\\nShort answers: {', '.join(answer_texts['short_answers_text'])}\")\n",
    "if answer_texts['yes_no_answer'] != 'NONE':\n",
    "    print(f\"\\nYes/No answer: {answer_texts['yes_no_answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nq_json(examples, output_file='processed_nq_data.json'):\n",
    "    \"\"\"Convert NQ examples into JSON file with validation statistics.\"\"\"\n",
    "    processed_data = []\n",
    "    validation_stats = {\n",
    "        'total_examples': 0,\n",
    "        'examples_with_both_answers': 0,\n",
    "        'examples_with_valid_containment': 0,\n",
    "        'problematic_examples': []\n",
    "    }\n",
    "    \n",
    "    for example in examples:\n",
    "        try:\n",
    "            processed = process_nq_example(example)\n",
    "            \n",
    "            # Track validation statistics\n",
    "            validation_stats['total_examples'] += 1\n",
    "            if processed['has_long_answer'] and processed['has_short_answer']:\n",
    "                validation_stats['examples_with_both_answers'] += 1\n",
    "                if processed['validation']['all_short_answers_in_long']:\n",
    "                    validation_stats['examples_with_valid_containment'] += 1\n",
    "                else:\n",
    "                    validation_stats['problematic_examples'].append({\n",
    "                        'example_id': processed['example_id'],\n",
    "                        'question': processed['question'],\n",
    "                        'problematic_short_answers': processed['validation']['problematic_short_answers']\n",
    "                    })\n",
    "            \n",
    "            processed_data.append(processed)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Print validation summary\n",
    "    print(\"\\nValidation Summary:\")\n",
    "    print(f\"Total examples processed: {validation_stats['total_examples']}\")\n",
    "    print(f\"Examples with both long and short answers: {validation_stats['examples_with_both_answers']}\")\n",
    "    print(f\"Examples with valid containment: {validation_stats['examples_with_valid_containment']}\")\n",
    "    print(f\"Number of problematic examples: {len(validation_stats['problematic_examples'])}\")\n",
    "    \n",
    "    if validation_stats['problematic_examples']:\n",
    "        print(\"\\nSample of problematic examples:\")\n",
    "        for example in validation_stats['problematic_examples'][:5]:  # Show first 5\n",
    "            print(f\"\\nQuestion: {example['question']}\")\n",
    "            print(f\"Problematic short answers: {example['problematic_short_answers']}\")\n",
    "    \n",
    "    # Write to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            'data': processed_data,\n",
    "            'validation_stats': validation_stats\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return processed_data, validation_stats\n",
    "\n",
    "# Usage:\n",
    "processed_data, validation_stats = create_nq_json(dev_nq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
